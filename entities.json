{
    "udc1024": {
        "abstract": "Locomotion,the most basic interaction in Virtual Environments (VE), enables users to move around the virtual world. Locomotion in Virtual Reality (VR) is a problem which has not been solved completely since existing techniques have a specific set of requirements and limitations. In addition, the uncertainty about the impact that virtual cues have on users perception complicates the development of better locomotion interfaces. A broadly applicable locomotion technique that is easy to use and addresses the issues of presence, cybersickness and fatigue has yet to be developed. Though optical flow and vestibular cues are dominant in navigation, other cues such as auditory, arm feedback, wind, etc. play a role. The proposed research aims to evaluate and improve upon a set of locomotion techniques for different modes of locomotion in virtual scenarios, as well as the transitions between them. The outcome measures of the evaluations of the different scenarios are usefulness for spatial orientation, presence, fatigue, cybersickness and user preference. The envisioned contribution of my thesis is research towards the design of a locomotion technique that is easy to use and addresses the shortcomings of current implementations.",
        "authors": [
            {
                "name": "Bhuvaneswari  Sarupuri, University of Canterbury"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual Reality",
            "Interaction techniques",
            "Locomotion"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Comfortable and Efficient Travel Techniques in VR",
        "type": "doctoral symposium presentation"
    },
    "udc1029": {
        "abstract": "People often run into barriers when doing creative tasks with software because it is difficult to translate goals into concrete actions. While expert-made tutorials, examples, and documentation abound online, finding the most relevant content and adapting it to one’s own situation and task is a challenge. My research introduces techniques for exposing relevant examples to novices in the context of their own workflows. These techniques are embodied in three systems. The first, RePlay, helps people find solutions when stuck by automatically locating relevant moments from expert-made videos. The second, DiscoverySpace, helps novices get started by mining and recommending expert-made software macros. The third, CritiqueKit, helps novices improve their work by providing ambient guidance and recommendations. Preliminary experiments with RePlay suggest that contextual video clips help people complete targeted tasks. Controlled experiments with DiscoverySpace and CritiqueKit demonstrate that software macros prevent novices from losing confidence, and ambient guidance improves novice output. My research illustrates the power of user communities to support creative learning.",
        "authors": [
            {
                "name": "C. Ailie Fraser, University of California, San Diego"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "creativity",
            "contextual assistance",
            "recommendations"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "The Right Content at the Right Time: Contextual Examples for Just-in-time Creative Learning",
        "type": "doctoral symposium presentation"
    },
    "udc1036": {
        "abstract": "With the increased popularity of cameras, more and more people are interested in learning photography. People are willing to invest in expensive cameras as a medium for their artistic expression, but few have access to in-person classes. Inspired by critique sessions common in in-person art practice classes, we propose design principles for creative learning. My dissertation research focuses on designing new interfaces and interactions that provide contextual in-camera feedback to aid users in learning visual elements of photography. We interactively visualize results of image processing algorithms as additional information for the user to make more informed and intentional decisions during capture. In this paper, we describe our design principles, and apply these principles in the design of two guided photography interfaces: one to explore lighting options for a portrait, and one to refine contents and composition of a photo. ",
        "authors": [
            {
                "name": "Jane L. E, Stanford University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "photography",
            "guidance",
            "context",
            "lighting",
            "composition"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Artistic Vision: Providing Contextual Guidance for Capture-Time Decisions",
        "type": "doctoral symposium presentation"
    },
    "udc1038": {
        "abstract": "Mobile and wearable computing are increasingly pervasive as people carry and use personal devices in everyday life. Screen sizes of such devices are becoming larger and smaller to accommodate both intimate and practical uses. Some mobile device screens are becoming larger to accommodate new experiences (e.g., phablet, tablet, eReader), whereas screen sizes on wearable devices are becoming smaller to allow them to fit into more places (e.g., smartwatch, wrist-band and eye-wear). However, these trends are making it difficult to use such devices with only one hand due to their placement, limited thumb reach and the fat-finger problem. This is especially true as there are many occasions when a user's other hand is occupied (encumbered) or not available.\nThis thesis work explores, creates and studies novel interaction techniques that enable effective single-hand usage on mobile and wearable devices, empowering users to achieve more with their smart devices when only one hand is available.",
        "authors": [
            {
                "name": "Hui-Shyong  Yeo, University of St Andrews"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Mobile devices, Wearable devices",
            "Single-handed interaction",
            "Subtle interaction",
            "Smartwatch",
            "Smart-ring",
            "Commercial off-the-shelf",
            ""
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Enabling Single-Handed Interaction in Mobile and Wearable Computing",
        "type": "doctoral symposium presentation"
    },
    "udc1042": {
        "abstract": "Wearable devices are becoming important computing devices to personal users. They have shown promising applications in multiple domains. However, designing interactions on smartwears remains challenging as the miniature sized formfactors limit both its input and output space. My thesis research proposes a new paradigm of Inherent Interaction on smartwears, with the idea of seeking interaction opportunities from users daily activities. This is to help bridging the gap between novel smartwear interactions and real-life experiences shared among users. This report introduces the concept of Inherent Interaction with my previous and current explorations in the category.",
        "authors": [
            {
                "name": "Teng  Han, University of Manitoba"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Wearable device",
            "inherent interaction",
            "gestural input and output",
            "acoustic sensing",
            "haptic",
            "interface design"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Designing Inherent Interactions on Wearable Devices",
        "type": "doctoral symposium presentation"
    },
    "udc1044": {
        "abstract": "Shape-changing interfaces match forms and haptics with functions and bring affordances to devices. I believe that shape-changing interfaces will be increasingly available to end-users in the future. To increase acceptance of shape-changing interfaces by end-users, we need to provide designers with design criteria and framework closely grounded on their current skills and needs. Also, we need to provide them with prototyping tools to enable quick assessment of ideas in the physical world. In this paper, I introduce the three threads of my Ph.D. research in the direction of providing the design tools. First, I advance existing shape-changing interface taxonomies to broaden design vocabulary and systemize design framework, based on the classification of everyday objects. Second, I conduct a study with end-users to suggest interaction techniques and design guidelines for shape-changing interfaces from their current practice. Lastly, I develop a physical prototyping tool for shape-changing interfaces to shorten prototyping iterations based on well-known Lego-like bricks.",
        "authors": [
            {
                "name": "Hyunyoung  Kim, Université Grenoble Alpes"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Shape-changing interfaces",
            "tangible user interfaces"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Fostering Design Process of Shape-Changing Interfaces",
        "type": "doctoral symposium presentation"
    },
    "udc1045": {
        "abstract": "As interactions move beyond the desktop, interactive behaviours (effects of actions as they happen, or once they happen) are becoming increasingly complex. This complexity is due to the variety of forms that objects might take, and the different inputs and sensors capturing information, and the ability to create nuanced responses to those inputs. Current interaction design tools do not support much of this rich behaviour authoring. In my work I create prototyping tools that examine ways in which designers can create interactive behaviours. Thus far, I have created two prototyping tools: Pineal and Astral, which examine how to create physical forms based on a smart object’s behaviour, and how to reuse existing desktop infrastructures to author different kinds of interactive behaviour. I also contribute conceptual elements, such as how to create smart objects using mobile devices, their sensors and outputs, instead of using custom electronic circuits, as well as devising evaluation strategies used in HCI toolkit research which directly informs my approach to evaluating my tools. ",
        "authors": [
            {
                "name": "David  Ledo, University of Calgary"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Prototyping",
            "prototyping tools",
            "design tools",
            "interaction design",
            "interactive behaviour",
            "mobile interfaces"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Designing Interactive Behaviours Beyond the Desktop",
        "type": "doctoral symposium presentation"
    },
    "udc1049": {
        "abstract": "The world is full of information, interfaces and environments that are inaccessible to blind people. When navigating indoors, blind people are often unaware of key visual information, such as posters, signs, and exit doors. When accessing specific interfaces, blind people cannot independently do so without at least first learning their layout and labeling them with sighted assistance. My work investigates interactive systems that integrates computer vision, on-demand crowdsourcing, and wearables to amplify the abilities of blind people, offering solutions for real-time environment and interface navigation. My work provides more options for blind people to access information and increases their freedom in navigating the world.",
        "authors": [
            {
                "name": "Anhong  Guo, Carnegie Mellon University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Accessibility",
            "blind",
            "crowdsourcing",
            "computer vision",
            "machine learning",
            "fabrication",
            "3D printing",
            "mobile",
            "wearable"
        ],
        "subtype": "doctoral symposium presentation",
        "title": "Crowd-AI Systems for Non-Visual Information Access in the Real World",
        "type": "doctoral symposium presentation"
    },
    "ude1006": {
        "abstract": "Although the exploration of variations is a key part of interface design, current processes for creating variations are mostly manual. We present Scout, a system that helps designers explore many variations rapidly through mixed-initiative interaction with high-level constraints and design feedback. Past constraint-based layout systems use low-level spatial constraints and mostly produce only a single design. Scout advances upon these systems by introducing high-level constraints based on design concepts (e.g. emphasis). With Scout, we have formalized several high-level constraints into their corresponding low-level spatial constraints to enable rapidly generating many designs through constraint solving and program synthesis.",
        "authors": [
            {
                "name": "Amanda M Swearngin, amaswea@cs.washington.edu"
            },
            {
                "name": "Andrew J Ko, University of Washington"
            },
            {
                "name": "James  Fogarty, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "user interfaces",
            "design",
            "prototyping",
            "variations"
        ],
        "subtype": "demo",
        "title": "Scout: Mixed-Initiative Exploration of Design Variations through High-Level Design Constraints",
        "type": "demo"
    },
    "ude1028": {
        "abstract": "We introduce a novel use for desktop 3D printers using actuators equipped in the printers. The actuators control an extruder and a build-plate mounted on a fused deposition modeling (FDM) 3D printer, moving them horizontally or vertically. Our technique enables actuation of 3D-printed objects on the build-plate by controlling the actuators, and people can interact with them by connecting interface devices to the 3D printer. In this work, we describe how to actuate printed objects using the actuators and present several objects illustrated by our technique.",
        "authors": [
            {
                "name": "Shohei  Katakura, Meiji University"
            },
            {
                "name": "Keita  Watanabe, Meiji University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "3D printing",
            "digital fabrication",
            "computer aided design",
            "interactive devices"
        ],
        "subtype": "demo",
        "title": "PrintMotion: Actuating Printed Objects Using Actuators Equipped in a 3D Printer",
        "type": "demo"
    },
    "ude1029": {
        "abstract": "We introduce MetaArms, wearable anthropomorphic robotic arms and hands with six degrees of freedom operated by the user's legs and feet. Our overall research goal is to re-imagine what our bodies can do with the aid of wearable robotics using a body-remapping approach. To this end, we present an initial exploratory case study. MetaArms' two robotic arms are controlled by the user's feet motion, and the robotic hands can grip objects according to the user's toes bending. Haptic feedback is also presented on the user's feet that correlate with the touched objects on the robotic hands, creating a closed-loop system. Using this system, users can experience an expanded number of arms interaction in which there legs are mapped into the artificial limbs. MetaArms provided initial indications for the sense of limbs alteration.",
        "authors": [
            {
                "name": "Tomoya  Sasaki, University of Tokyo"
            },
            {
                "name": "Mhd Yamen  Saraiji, Keio University"
            },
            {
                "name": "Kouta  Minamizawa, Keio University"
            },
            {
                "name": "Masahiko  Inami, University of Tokyo"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Artificial Limbs",
            "Feet Interactions",
            "Body Remapping",
            "Body Schema",
            "Augmented Arms",
            "Human Enhancement"
        ],
        "subtype": "demo",
        "title": "MetaArms: Body Remapping Using Feet-Controlled Artificial Arms",
        "type": "demo"
    },
    "ude1030": {
        "abstract": "",
        "authors": [
            {
                "name": "Michael  Wessely, wessely@lri.fr"
            },
            {
                "name": "France Wendy Elizabeth, mackay@lri.fr"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "demo",
        "title": "Shape-Aware Material: Interactive Fabrication with ShapeMe",
        "type": "demo"
    },
    "ude1031": {
        "abstract": "Prototyping interactive objects with personal fabrication tools like 3D printers requires the maker to create subsequent design artifacts from scratch which produces unnecessary waste and does not allow to reuse functional components.  We present Interactive Tangrami, paper-folded and reusable building blocks (Tangramis) that can contain various sensor input and visual output capabilities. We propose a digital design toolkit that lets the user plan the shape and functionality of a design piece. The software manages the communication to the physical artifact and streams the interaction data via the Open Sound protocol (OSC) to an application prototyping system (e.g. MaxMSP). The building blocks are fabricated digitally with a rapid and inexpensive ink-jet printing method. Our systems allows to prototype physical user interfaces within minutes and without knowledge of the underlying technologies. We demo its usefulness with two application examples.",
        "authors": [
            {
                "name": "Michael  Wessely, INRIA, Univ Paris-Sud, CNRS, Universite Paris-Saclay"
            },
            {
                "name": "Nadiya  Morenko, Academy of Fine Arts"
            },
            {
                "name": "Jürgen  Steimle, Saarland University"
            },
            {
                "name": "Michael  Schmitz, Academy of Fine Arts"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "personal fabrication",
            "modular fabrication",
            "building blocks",
            "tangibles",
            "design tool",
            "sensing technologies",
            "printed electronics"
        ],
        "subtype": "demo",
        "title": "Interactive Tangrami: Rapid Prototyping with Modular Paper-folded Electronics",
        "type": "demo"
    },
    "ude1032": {
        "abstract": "In this demonstration, we introduce Face/On, an embedded feedback device that leverages the contact area between the user's face and a virtual reality (VR) head-mounted display (HMD) to provide rich haptic feedback in virtual environments (VEs). Head-worn haptic feedback devices have been explored in previous work to provide directional cues via grids of actuators and localized feedback on the users' skin. Most of these solutions were immersion breaking due to their encumbering and uncomfortable design and build around a single actuator type, thus limiting the overall fidelity and flexibility of the haptic feedback. We present Face/On, a VR HMD face cushion with three types of discreetly embedded actuators that provide rich haptic feedback without encumbering users with invasive instrumentation on the body. By combining vibro-tactile and thermal feedback with electrical muscle stimulation (EMS), Face/On can simulate a wide range of scenarios and benefit from synergy effects between these feedback types.",
        "authors": [
            {
                "name": "Dennis  Wolf, Ulm University"
            },
            {
                "name": "Leo  Hnatek, Ulm University"
            },
            {
                "name": "Enrico  Rukzio, Ulm University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "VR",
            "multi-modal",
            "heat",
            "vibration",
            "haptic",
            "feedback"
        ],
        "subtype": "demo",
        "title": "Face/On: Actuating the Facial Contact Area of a Head-Mounted Display for Increased Immersion",
        "type": "demo"
    },
    "ude1033": {
        "abstract": "Virtual reality (VR) using a head-mounted display (HMD) have been rapidly becoming popular. Lots of HMD products and various VR applications such as games, training tools and communication services have been released in recent years. However, there is a well-known problem that the user's face is covered by the HMD preventing the facial expression from being captured. This strongly restricts VR applications. For example, users wearing HMDs normally cannot exchange their face images. This degrades communication quality in virtual spaces because facial expressions are an important element of human communication.",
        "authors": [
            {
                "name": "Mariko  Chiba, NTT DOCOMO"
            },
            {
                "name": "Wataru  Yamada, NTT DOCOMO"
            },
            {
                "name": "Hiroyuki  Manabe, NTT DOCOMO"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "virtual reality",
            "head-mounted display",
            "infrared pass filter",
            "face-capturing HMD",
            "telecommunication"
        ],
        "subtype": "demo",
        "title": "Transparent Mask: Face-Capturing Head-Mounted Display with IR Pass Filters",
        "type": "demo"
    },
    "ude1037": {
        "abstract": "In this demonstration, as an attempt of a new haptic presentation method for objects in virtual reality (VR) environment, we show a device that presents the haptic sensation of the fingertip on the forearm, not on the fingertip. This device adopts a five-bar linkage mechanism and it is possible to present the strength, direction of force. Compared with a fingertip mounted type displays, it is possible to address the issues of their weight and size which hinder the free movement of fingers. We have confirmed that the experiences in the VR environment is improved compared with without haptics cues situation regardless of without presenting haptics information directly to the fingertip.",
        "authors": [
            {
                "name": "Taha  Moriyama, University of Electro-Communications"
            },
            {
                "name": "Takuto  Nakamura, University of Electro-Communications"
            },
            {
                "name": "Hiyoruki  Kajimoto, University of Electro-Communications"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Human-centered computing",
            " Haptic devices"
        ],
        "subtype": "demo",
        "title": "Wearable Haptic Device that Presents the Haptics Sensation Corresponding to Three Fingers on the Forearm",
        "type": "demo"
    },
    "ude1041": {
        "abstract": "With the spread of VR experiences using HMD, many proposals have been made to improve the experiences by providing tactile information to the fingertips. However, there are problems, such as difficulty attaching and detaching the devices and hindrances to free finger movement. To solve these issues, we developed “Haptopus,” which embeds a tactile display in the HMD and presents tactile sensations to the face. In this paper, we conducted a preliminary investigation on the best suction pressure and compared Haptopus to conventional tactile presentation approaches. As a result, we confirmed that Haptopus improves the quality of the VR experience.",
        "authors": [
            {
                "name": "Takayuki  Kameoka, The University of Electro-Communications"
            },
            {
                "name": "Yuki  Kon, The University of Electro-Communications"
            },
            {
                "name": "Takuto  Nakamura, The University of Electro-Communications"
            },
            {
                "name": "Hiroyuki  Kajimoto, The University of Electro-Communications"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Haptopus",
            "suction stimulus",
            "haptics HMD",
            "virtual reality"
        ],
        "subtype": "demo",
        "title": "Haptopus : Haptic VR Experience Using Suction Mechanism Embedded in Head-mounted Display",
        "type": "demo"
    },
    "ude1043": {
        "abstract": "Herein, we propose “unlimited electric gum,” an electric taste device that will enable users to perceive taste for as long the user is chewing the gum. We developed an in-mouth type novel electric taste-imparting apparatus using a piezoelectric element so that the piezoelectric effect is stimulated by chewing. This enabled the design of a device that does not require cables around a user's lips or batteries in their mouth. In this paper, we introduce this device and report our experimental and exhibition results.",
        "authors": [
            {
                "name": "Naoshi  Ooba, Meiji University"
            },
            {
                "name": "Kazuma  Aoyama, University of Tokyo"
            },
            {
                "name": "Hiromi  Nakamura, Meiji University"
            },
            {
                "name": "Homei  Miyashita, Meiji University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Electric taste",
            "Chewing activities",
            "Virtual reality"
        ],
        "subtype": "demo",
        "title": "Unlimited Electric Gum: A Piezo-based Electric Taste Apparatus Activated by Chewing",
        "type": "demo"
    },
    "ude1046": {
        "abstract": "In this paper, we propose a method to create 3D inflatable objects by laminating plastic layers. AccordionFab is a fabrication method in which the user can prototype multi-layered inflatable structures rapidly with a common laser cutter. Our key finding is that it is possible to selectively weld the two uppermost plastic sheets out of the stacked sheets by defocusing the laser and inserting the heat-resistant paper below the desired welding layer.  As the contribution of our research, we investigated the optimal distance between the lens and the workpiece for cutting and welding and developed an attachment which supports welding process. Next, we developed a mechanism of changing the thickness and bending angle of multi-layered objects and created a simulation software. Using these techniques, the user can create various prototypes such as personal furniture that fits user's body and packing containers that fit the contents.",
        "authors": [
            {
                "name": "Junichi  Yamaoka, The University of Tokyo"
            },
            {
                "name": "Kazunori  Nozawa, The University of Tokyo"
            },
            {
                "name": "Shion  Asada, The University of Tokyo"
            },
            {
                "name": "Ryuma  Niiyama, The University of Tokyo"
            },
            {
                "name": "Yoshihiro  Kawahara, The University of Tokyo"
            },
            {
                "name": "Yasuaki  Kakehi, The University of Tokyo"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Fabrication",
            "Prototyping/Implementation",
            "Creativity Support",
            "Smart materials"
        ],
        "subtype": "demo",
        "title": "AccordionFab: Fabricating Inflatable 3D Objects by Laser Cutting and Welding Multi-Layered Sheets",
        "type": "demo"
    },
    "ude1047": {
        "abstract": "Recent years saw an explosion in Augmented Reality (AR) experiences for consumers. These experiences can be classified based on the scale of the interactive area (room vs city/global scale) , or the fidelity of the experience (high vs low). Experiences that target large areas, such as campus or world scale, commonly have only rudimentary interactions with the physical world, and suffer from registration errors and jitter. We classify these experiences as large scale and low fidelity. On the other hand, various room sized experiences feature realistic interaction of virtual content with the real world. We classify these experiences as small scale and high fidelity. Our work is the first to explore the domain of large scale high fidelity (LSHF) AR experiences. We build upon the small scale high fidelity capabilities of the Microsoft HoloLens to allow LSHF interactions. We demonstrate the capabilities of our system with a game specifically designed for LSHF interactions, handling many challenges and limitations unique to the domain of LSHF AR through the game design. Our contributions are twofold:  - The lessons learned during the design and development of a system capable of LSHF AR interactions. - Identification of a set of reusable game elements specific to LSHF AR, including mechanisms for addressing spatiotemporal\ninconsistencies and crowd control. We believe our contributions will be fully applicable not only to games, but all LSHF AR experiences.",
        "authors": [
            {
                "name": "Damien Constantine Rompapas, Nara Institute of Science and Technology"
            },
            {
                "name": "Christian  Sandor, Nara Institute of Science and Technology"
            },
            {
                "name": "Alexander  Plopski, Nara Institute of Science and Technology"
            },
            {
                "name": "Daniel  Saakes, Korea Advanced Institute of Science and Technology"
            },
            {
                "name": "Dong Hyeok Yun, Korea Advanced Institute of Science and Technology"
            },
            {
                "name": "Takafumi  Taketomi, Nara Institute of Science and Technology"
            },
            {
                "name": "Hirokazu  Kato, Nara Institute of Science and Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Augmented Reality",
            "High Fidelity",
            "Game Design",
            "Large Scale Interaction"
        ],
        "subtype": "demo",
        "title": "HoloRoyale: A Large Scale High Fidelity Augmented Reality Game",
        "type": "demo"
    },
    "ude1048": {
        "abstract": "Communication between screens and cameras has attracted attention as a ubiquitous information source, motivated by the widespread use of smartphones and the increase of public advertising and information screens. We propose embedding matrix barcodes into images projected on displays by utilizing imperceptible color vibration. This approach maintains the visual experience as the barcodes are imperceptible and can be implemented on almost any display and camera for the technology to be pervasive. In fact, the color vibration can be generated by ordinary 60 Hz LCDs and captured by 120 fps smartphone cameras. To illustrate the technology capabilities, we present scenarios of potential practical applications.",
        "authors": [
            {
                "name": "Satoshi  Abe, abe@nae-lab.org"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "screen-camera communication",
            "data embedding",
            "imperceptible color vibration",
            "ubiquitous computing"
        ],
        "subtype": "demo",
        "title": "Screen–Camera Communication via Matrix Barcode Utilizing Imperceptible Color Vibration",
        "type": "demo"
    },
    "ude1054": {
        "abstract": "In this demonstration, we propose OptRod, constructing interactive surface with multiple functions and flexible shape by projected image. A PC generates images as control signals and projects them to the bottom of OptRods by a projector or LCD. An OptRod receives the light and converts its brightness into a control signal for the attached output device. By using multiple OptRods, the PC can simultaneously operate many output devices without any signal lines. Moreover, we can arrange surfaces of various shapes easily by combining multiple OptRods. OptRod supports various functions by replacing the device unit connected to OptRod.",
        "authors": [
            {
                "name": "Ryo  Shirai, Osaka University"
            },
            {
                "name": "Yuichi  Itoh, Osaka University"
            },
            {
                "name": "Shori  Ueda, Osaka University"
            },
            {
                "name": "Takao  Onoye, Osaka University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Actuated surface",
            "shape-free display",
            "multi functional display",
            "controlled by image",
            "visible light communication"
        ],
        "subtype": "demo",
        "title": "OptRod: Constructing Interactive Surface with Multiple Functions and Flexible Shape by Projected Image",
        "type": "demo"
    },
    "ude1055": {
        "abstract": "This demonstration corresponds to our previous paper, which deals with our finding that a proprioceptive force sensation can be presented by electrical stimulation from the skin surface to the tendon region (Tendon Electrical Stimulation: TES). We showed that TES can elicit a force sensation, and adjusting the current parameters can control the amount of the sensation. Unlike electrical muscle stimulation (EMS), which can also present force sensation by stimulating motor nerves to contract muscles, TES is thought to present a proprioceptive force sensation by stimulating receptors or sensory nerves responsible for recognizing the magnitude of the muscle contraction existing inside the tendon. In the demo, we offer the occasion for trying TES.",
        "authors": [
            {
                "name": "Akifumi  Takahashi, The University of Electro-Communications"
            },
            {
                "name": "Kenta  Tanabe, The University of Electro-Communications"
            },
            {
                "name": "Hiroyuki  Kajimoto, The University of Electro-Communications"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Tendon Electrical Stimulation",
            "Haptic Interface",
            "Proprioceptive Force Sensation"
        ],
        "subtype": "demo",
        "title": "Haptic Interface Using Tendon Electrical Stimulation",
        "type": "demo"
    },
    "ude1056": {
        "abstract": "We propose to equip smartphone-based HMDs (SbHMDs) with an additional touch pad.  SbHMDs are a low cost approach to allowing users to experience virtual reality (VR). Current SbHMDs, however, provide poor input functionality and sometimes external devices are necessary to enhance the VR experience.  Our proposal uses frustrated total internal reflection (FTIR) to realize a touch pad on the external surfaces of the HMD case; no special devices are needed.  As simple FTIR approaches do not suit SbHMDs due to the spatial relation between camera and light, we design an arrangement of acrylic plates and mirror suitable for smartphone's built-in camera and torch-light. It extends the input vocabulary SbHMDs to include touch location, gestures, and also pressure.",
        "authors": [
            {
                "name": "Takuya  Kitade, NTT DOCOMO"
            },
            {
                "name": "Wataru  Yamada, NTT DOCOMO"
            },
            {
                "name": "Hiroyuki  Manabe, NTT DOCOMO"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Mobile HMD",
            "Virtual Reality",
            "frustrated total internal reflection (FTIR)"
        ],
        "subtype": "demo",
        "title": "FTIR-based Touch Pad for Smartphone-based HMD Enhancement",
        "type": "demo"
    },
    "ude1057": {
        "abstract": "In this paper, we introduce the Immersive Bubble Chart, a visualization for hierarchical datasets presented in a virtual reality (VR) world. Users get immersed into the visualization and interact with the bubbles using gestures with a view to overcoming some limitations of 2D visualizations due to the capabilities and interaction affordances of the devices. The technological advances in VR give the possibility to design malleable and extensible representations and more natural and engaging interactions. Using the Oculus Touch controllers, the users can grab and move the bubbles, throw them away or bump two of them for creating a cluster. We have tested the Immersive Bubble Chart with the hierarchical clusters of semantically related terms generated from Twitter.",
        "authors": [
            {
                "name": "Teresa  Onorati, Universidad Carlos III de Madrid"
            },
            {
                "name": "Paloma  Díaz, Universidad Carlos III de Madrid"
            },
            {
                "name": "Telmo  Zarraonandia, Universidad Carlos III de Madrid"
            },
            {
                "name": "Ignacio  Aedo, Universidad Carlos III de Madrid"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Immersive Analytics",
            "Virtual Reality",
            "Information Visualization"
        ],
        "subtype": "demo",
        "title": "The Immersive Bubble Chart: a Semantic and Virtual Reality Visualization for Big Data",
        "type": "demo"
    },
    "ude1062": {
        "abstract": "In collaborative virtual environments, users must often perform tasks requiring coordinated action between multiple parties. Some cases are symmetric, in which users work together on equal footing, while others are asymmetric, in which one user may have more experience or capabilities than another (e.g., one may guide another in completing a task). We present a multi-user virtual reality system that supports interactions of both these types. Two collaborating users, whether co-located or remote, simultaneously manipulate the same virtual objects in a physics simulation, in tasks that require low latency networking to perform successfully. We are currently applying this approach to motor rehabilitation, in which a therapist and patient work together.",
        "authors": [
            {
                "name": "Carmine  Elvezio, Columbia University"
            },
            {
                "name": "Frank  Ling, Columbia University"
            },
            {
                "name": "Jen-Shuo  Liu, Columbia University"
            },
            {
                "name": "Steven  Feiner, Columbia University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual Reality",
            "Collaboration",
            "Games",
            "Rehabilitation"
        ],
        "subtype": "demo",
        "title": "Collaborative Virtual Reality for Low-Latency Interaction",
        "type": "demo"
    },
    "ude1064": {
        "abstract": "We present a wearable soft exoskeleton sleeve based on PGM. The sleeve consists of 4 PGMs is controlled by a computing system and can actuate 4 different movements (hand extension, flexion, pronation and supination). Depending on how strong the actuation is, the user feels a slight force (haptic feedback) or the hand moves (if the users relaxes the muscles). The paper gives details about the system implementation, the interaction space and some ideas about application scenarios.",
        "authors": [
            {
                "name": "Takashi  Goto, Graduate School of Media Design, Keio University"
            },
            {
                "name": "Swagata  Das, Graduate School of Engineering, Hiroshima University"
            },
            {
                "name": "Yuichi  Kurita, Graduate School of Engineering, Hiroshima University"
            },
            {
                "name": "Kai  Kunze, Graduate School of Media Design, Keio University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Haptics and Haptic Interfaces",
            "Wearable Devices",
            "Pneumatic Artificial Muscles (PAMs)"
        ],
        "subtype": "demo",
        "title": "Artificial Motion Guidance: an Intuitive Device based on Pneumatic Gel Muscle (PGM)",
        "type": "demo"
    },
    "ude1066": {
        "abstract": "In this demonstration we introduce spinVR, a seated locomotion approach based around stimulating the user's vestibular system using a rotational impulse to induce the perception of linear self-motion. Currently, most approaches for locomotion in VR use either concepts like teleportation for traveling longer distances or present a virtual motion that creates a visual-vestibular conflict, which is assumed to cause simulator sickness. With our platform we evaluated two designs for using the rotation of a motorized swivel chair to alleviate this, emph{wiggle} and emph{impulse}. Our evaluation showed that emph{impulse}, using short rotation bursts matched with the visual acceleration, can significantly reduce simulator sickness and increase the perception of self-motion compared to no physical motion.",
        "authors": [
            {
                "name": "Thomas  Dreja, Ulm University"
            },
            {
                "name": "Michael  Rietzler, Ulm University"
            },
            {
                "name": "Teresa  Hirzle, Ulm University"
            },
            {
                "name": "Jan  Gugenheimer, Ulm University"
            },
            {
                "name": "Julian  Frommel, Ulm University"
            },
            {
                "name": "Enrico  Rukzio, Ulm University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual Reality",
            "Simulator Sickness",
            "Vection",
            "Swivel Chair"
        ],
        "subtype": "demo",
        "title": "A Demonstration of VRSpinning: Exploring the Design Space of a 1D Rotation Platform to Increase the Perception of Self-Motion in VR",
        "type": "demo"
    },
    "ude1067": {
        "abstract": "Visual blends are an advanced graphic design technique to draw users' attention to a message. They blend together two objects in a way that is novel and useful in conveying a message symbolically. This demo presents an interactive pipeline for creating visual blends that follows the iterative design process. Our pipeline decomposes the process into both computational techniques and human microtasks. It allows users to collaboratively generate visual blends with steps involving brainstorming, synthesis, and iteration. Our demo allows individual users to see how existing visual blends were made, edit or improve existing visual blends, and create new visual blends.",
        "authors": [
            {
                "name": "Lydia B Chiton, Columbia University"
            },
            {
                "name": "Savvas  Petridis, Columbia University"
            },
            {
                "name": "Maneesh  Agrawala, Stanford University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Computational design",
            "collaboration",
            "workflow",
            "visual blends"
        ],
        "subtype": "demo",
        "title": "An Interactive Pipeline for Creating Visual Blends",
        "type": "demo"
    },
    "ude1068": {
        "abstract": "We present a textile sensor, capable of detecting multi-touch and multi-pressure input on non-planar surfaces and demonstrate how such sensors can be fabricated and integrated into pressure stabilized membrane envelopes (i.e. inflatables). Our sensor design is both stretchable and flexible/bendable and can thus conform to a large variety of three-dimensional surface geometries, including shape-changing surfaces. We briefly outline an approach for basic signal acquisition from such sensors and how they can be leveraged to measure internal air-pressure of inflatable objects without a need for specialized air-pressure sensors. We further demonstrate how rigid electronic components can be mounted inside malleable inflatable objects without the need for dedicated enclosures for mechanical protection.",
        "authors": [
            {
                "name": "Kristian  Gohlke, kristian.gohlke@uni-weimar.de"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "textile sensor",
            "multi-touch",
            "pressure",
            "membrane envelopes",
            "shape-change",
            "fluidic interfaces",
            "inflatables",
            "pneumatibles"
        ],
        "subtype": "demo",
        "title": "A Stretch-Flexible Textile Multitouch Sensor for User Input on Inflatable Membrane Structures & Non-Planar Surfaces",
        "type": "demo"
    },
    "ude1070": {
        "abstract": "We demonstrate a haptic feedback method to generate multiple virtual textures on analog buttons of the gamepad. The method utilizes the haptic illusion evoked from proper haptic cues in respect of the analog button's movement to change the perceived physical property of the button. Two types of analog buttons, joystick and trigger button on the gamepad is augmented with localized haptic feedback. We implemented two virtual textures for each type of analog button, and these textures could be programmatically controlled reflecting the dynamic game situations. We also demonstrate a two-player shooter game to show the dynamic texture representation of customized gamepad could enrich the game experience.",
        "authors": [
            {
                "name": "Youngbo Aram Shim, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Geehyuk  Lee, Korea Advanced Institute of Science & Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Haptic",
            "haptic texture",
            "gamepad",
            "joystick",
            "analog button",
            "haptic illusion"
        ],
        "subtype": "demo",
        "title": "Demonstrating Gamepad with Programmable Haptic Texture Analog Buttons",
        "type": "demo"
    },
    "ude1080": {
        "abstract": "We present six rotary knobs, each with a distinct shape, that provide haptic force feedback on rotation. The knob shapes were evaluated in relation to twelve haptic feedback stimuli. The stimuli were designed as a combination of the most relevant perceptual parameters of force feedback; acceleration, friction, detent amplitude and spacing. The results indicate that there is a relationship between the shape of a knob and its haptic feedback. The perceived functionality can be dynamically altered by changing its shape and haptic feedback. This work serves as basis for the design of dynamic interface controls that can adapt their shape and haptic feel to the content that is controlled.  In our demonstration, we show the six distinct knobs shapes with the different haptic feedback stimuli. Attendees can experience the interaction with the different knob shapes in relation the stimuli and design stimuli with a graphical editor.\n",
        "authors": [
            {
                "name": "Anke  Van Oosterhout, a.v.oosterhout@cs.au.dk"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Physical control knob",
            "haptic force feedback",
            "affordance"
        ],
        "subtype": "demo",
        "title": "Knobology 2.0: Giving Shape to the Haptic Force Feedback of Interactive Knobs",
        "type": "demo"
    },
    "ude1081": {
        "abstract": "We introduce programmable material and electro-mechanical control to enable a set of hybrid watch user interfaces that symbiotically leverage the joint strengths of electro-mechanical hands and a dynamic watch dial. This approach enables computation and connectivity with existing materials to preserve the inherent physical qualities and abilities of traditional analog watches. We augment the watch's mechanical hands with micro-stepper motors for control, positioning and mechanical expressivity. We extend the traditional watch dial with programmable pigments for non-emissive dynamic patterns. Together, these components enable a unique set of interaction techniques and user interfaces beyond their individual capabilities.",
        "authors": [
            {
                "name": "Alex  Olwal, olwal@google.com"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Wearable computing",
            "programmable material",
            "E-ink",
            "actuation",
            "analog watches",
            "hybrid watches",
            "smartwatches"
        ],
        "subtype": "demo",
        "title": "Hybrid Watch User Interfaces: Collaboration Between Electro-Mechanical Components and Analog Materials",
        "type": "demo"
    },
    "ufp1003": {
        "abstract": "This paper presents FacePush, a Head-Mounted Display (HMD) integrated with a pulley system to generate normal forces on a user’s face in virtual reality (VR). The mechanism of FacePush is obtained by shifting torques provided by two motors that press upon a user’s face via utilization of a pulley system. FacePush can generate normal forces of varying strengths and apply those to the surface of the face. To inform our design of FacePush for noticeable and discernible normal forces in VR applications, we conducted two studies to iden- tify the absolute detection threshold and the discrimination threshold for users’ perception. After further consideration in regard to user comfort, we determined that two levels of force, 2.7 kPa and 3.375 kPa, are ideal for the development of the FacePush experience via implementation with three applications which demonstrate use of discrete and continuous normal force for the actions of boxing, diving, and 360 guidance in virtual reality. In addition, with regards to a virtual boxing application, we conducted a user study evaluating the user experience in terms of enjoyment and realism and collected the user’s feedback.",
        "authors": [
            {
                "name": "Hong-Yu  Chang, National Chiao Tung University"
            },
            {
                "name": "Wen-Jie  Tseng, National Chiao Tung University"
            },
            {
                "name": "Chia-En  Tsai, National Chiao Tung University"
            },
            {
                "name": "Hsin-Yu  Chen, National Chiao Tung University"
            },
            {
                "name": "Roshan Lalintha Peiris, Keio University"
            },
            {
                "name": "Liwei  Chan, National Chiao Tung University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual reality",
            "Normal force",
            "Facial haptics",
            "Head-mounted displays"
        ],
        "subtype": "paper",
        "title": "FacePush: Introducing Normal Force on Face with Head-Mounted Displays",
        "type": "paper"
    },
    "ufp1009": {
        "abstract": "Keyboard shortcuts can be more efficient than graphical input, but they are underused by most users. To alleviate this, we present \"Guided Finger-Aware Shortcuts\"to reduce the gulf between graphical input and shortcut activation. The interaction technique works by recognising when a special hand posture is used to press a key, then allowing secondary finger movements to select among related shortcuts if desired. Novice users can learn the mappings through dynamic visual guidance revealed by holding a key down, but experts can trigger shortcuts directly without pausing. Two variations are described: FingerArc uses the angle of the thumb, and FingerChord uses a second key press. The techniques are motivated by an interview study identifying factors hindering the learning, use, and exploration of keyboard shortcuts. A controlled comparison with conventional keyboard shortcuts shows the techniques encourage overall shortcut usage, make interaction faster, less error-prone, and provide advantages over simply adding visual guidance to standard shortcuts.",
        "authors": [
            {
                "name": "Jingjie  Zheng, University of Waterloo & Google"
            },
            {
                "name": "Blaine  Lewis, University of Waterloo"
            },
            {
                "name": "Jeff  Avery, University of Waterloo"
            },
            {
                "name": "Daniel  Vogel, dvogel@uwaterloo.ca"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "keyboard shortcuts",
            "finger identification"
        ],
        "subtype": "paper",
        "title": "FingerArc and FingerChord: Supporting Novice to Expert Transitions with Guided Finger-Aware Shortcuts",
        "type": "paper"
    },
    "ufp1013": {
        "abstract": "Ultrasound manipulation is growing in popularity in the HCI community with applications in haptics, on-body interaction, and levitation-based displays. Most of these applications share two key limitations: a) the complexity of the sound fields that can be produced is limited by the physical size of the transducers, and b) no obstacles can be present between the transducers and the control point. We present SoundBender, a hybrid system that overcomes these limitations by combining the versatility of phased arrays of Transducers (PATs) with the precision of acoustic metamaterials. In this paper, we explain our approach to design and implement such hybrid modulators (i.e. to create complex sound fields) and methods to manipulate the field dynamically (i.e. stretch, steer). We demonstrate our concept using self-bending beams enabling both levitation and tactile feedback around an obstacle and present example applications enabled by SoundBender.",
        "authors": [
            {
                "name": "Mohd Adili  Norasikin, University of Sussex"
            },
            {
                "name": "Diego  Martinez Plasencia, University of Sussex"
            },
            {
                "name": "Spyros  Polychronopoulos, University of Sussex"
            },
            {
                "name": "Gianluca  Memoli, University of Sussex"
            },
            {
                "name": "Yutaka  Tokuda, University of Sussex"
            },
            {
                "name": "Sriram  Subramanian, University of Sussex"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Acoustic manipulation",
            "Metamaterials",
            "Self-bending beams"
        ],
        "subtype": "paper",
        "title": "SoundBender: Dynamic Acoustic Control Behind Obstacles",
        "type": "paper"
    },
    "ufp1014": {
        "abstract": "A wide variety of tools for creating physical computing systems have been developed, but getting started in this domain remains challenging for novices. In this paper, we introduce test-driven physical computing tutorials, a novel application of interactive tutorial systems to better support users in building and programming physical computing systems. These tutorials inject interactive tests into the tutorial process to help users verify and understand individual steps before proceeding. We begin by presenting a taxonomy of the types of tests that can be incorporated into physical computing tutorials. We then present ElectroTutor, a tutorial system that implements a range of tests for both the software and physical aspects of a physical computing system. A user study suggests that ElectroTutor can improve users’ success and confidence when completing a tutorial, and save them time by reducing the need to backtrack and troubleshoot errors made on previous tutorial steps.",
        "authors": [
            {
                "name": "Jeremy  Warner, University of California, Berkeley"
            },
            {
                "name": "Ben  Lafreniere, Autodesk Research"
            },
            {
                "name": "George  Fitzmaurice, Autodesk Research"
            },
            {
                "name": "Tovi  Grossman, Autodesk Research & University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "physical computing",
            "software learning",
            "reactive tutorials"
        ],
        "subtype": "paper",
        "title": "ElectroTutor : Test-Driven Physical Computing Tutorials",
        "type": "paper"
    },
    "ufp1015": {
        "abstract": "We present a system that visualizes complex human motion via 3D motion sculptures—a representation that conveys the 3D structure swept by a human body as it moves through space. Our system computes a motion sculpture from an input video, and then embeds it back into the scene in a 3D-aware fashion. The user may also explore the sculpture directly in 3D or physically print it. Our interactive interface allows users to customize the sculpture design, for example, by selecting materials and lighting conditions.\n\nTo provide this end-to-end workflow, we introduce an algorithm\nthat estimates a human’s 3D geometry over time from a\nset of 2D images, and develop a 3D-aware image-based rendering\napproach that inserts the sculpture back into the original\nvideo. By automating the process, our system takes motion\nsculpture creation out of the realm of professional artists, and\nmakes it applicable to a wide range of existing video material.\n\nBy conveying 3D information to users, motion sculptures reveal\nspace-time motion information that is difficult to perceive\nwith the naked eye, and allow viewers to interpret how different\nparts of the object interact over time. We validate the\neffectiveness of motion sculptures with user studies, finding\nthat our visualizations are more informative about motion than\nexisting stroboscopic and space-time visualization methods.",
        "authors": [
            {
                "name": "Xiuming  Zhang, Massachusetts Institute of Technology"
            },
            {
                "name": "Tali  Dekel, Google Research"
            },
            {
                "name": "Tianfan  Xue, Google Research"
            },
            {
                "name": "Andrew  Owens, University of Calfornia, Berkeley"
            },
            {
                "name": "Qiurui  He, Massachusetts Institute of Technology"
            },
            {
                "name": "Jiajun  Wu, Massachusetts Institute of Technology"
            },
            {
                "name": "Stefanie  Mueller, Massachusetts Institute of Technology"
            },
            {
                "name": "William T. Freeman, Massachusetts Institute of Technology & Google Research"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1015.jpg",
        "keywords": [
            "Motion estimation and visualization",
            "image-based rendering"
        ],
        "subtype": "paper",
        "title": "MoSculp: Interactive Visualization of Shape and Time",
        "type": "paper"
    },
    "ufp1022": {
        "abstract": "Advanced Driver Assistance Systems (ADAS) come equipped on most modern vehicles and are intended to assist the driver and enhance the driving experience through features such as lane keeping system and adaptive cruise control. However, recent studies show that few people utilize these features for several reasons. First, ADAS features were not common until recently. Second, most users are unfamiliar with these features and do not know what to expect. Finally, the interface for operating these features is not intuitive. To help drivers understand ADAS features, we present a conversational in-vehicle digital assistant that responds to drivers' questions and commands in natural language. With the system prototyped herein, drivers can ask questions or command using unconstrained natural language in the vehicle, and the assistant trained by using advanced machine learning techniques, coupled with access to vehicle signals, responds in real-time based on conversational context. Results of our system prototyped on a production vehicle are presented, demonstrating its effectiveness in improving driver understanding and usability of ADAS.",
        "authors": [
            {
                "name": "Shih-Chieh  Lin, University of Michigan"
            },
            {
                "name": "Chang-Hong  Hsu, University of Michigan"
            },
            {
                "name": "Walter  Talamonti, Ford Motor Company"
            },
            {
                "name": "Yunqi  Zhang, University of Michigan"
            },
            {
                "name": "Steve  Oney, University of Michigan"
            },
            {
                "name": "Jason  Mars, University of Michigan"
            },
            {
                "name": "Lingjia  Tang, University of Michigan"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Advanced Driver Assistance System",
            "Automobile Interface",
            "Voice Interface",
            "Machine Learning",
            ",Advanced Driver Assistance System",
            "Automobile Interface",
            "Voice Interface",
            "Machine Learning"
        ],
        "subtype": "paper",
        "title": "Adasa: A Conversational In-Vehicle Digital Assistant for Advanced Driver Assistance Features",
        "type": "paper"
    },
    "ufp1044": {
        "abstract": "We present Ownershift, an interaction technique for easing\noverhead manipulation in virtual reality, while preserving the\nillusion that the virtual hand is the user’s own hand. In contrast\nto previous approaches, this technique does not alter\nthe mapping of the virtual hand position for initial reaching\nmovements towards the target. Instead, the virtual hand space\nis only shifted gradually if interaction with the overhead target\nrequires an extended amount of time. While users perceive\ntheir virtual hand as operating overhead, their physical\nhand moves gradually to a less strained position at waist level.\nWe evaluated the technique in a user study and show that\nOwnershift significantly reduces the physical strain of overhead\ninteractions, while only slightly reducing task performance\nand the sense of body ownership of the virtual hand.",
        "authors": [
            {
                "name": "Tiare  Feuchtner, Aarhus University"
            },
            {
                "name": "Jörg  Müller, University of Bayreuth"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1044.jpg",
        "keywords": [
            "body ownership",
            "virtual hand illusion"
        ],
        "subtype": "paper",
        "title": "Ownershift: Facilitating Overhead Interaction in Virtual Reality with an Ownership-Preserving Hand Space Shift",
        "type": "paper"
    },
    "ufp1047": {
        "abstract": "We present SynchronizAR, an approach to spatially register multiple SLAM devices together without sharing maps or involving external tracking infrastructures. SynchronizAR employs a distance based indirect registration which resolves the transformations between the separate SLAM coordinate systems. We attach an Ultra-Wide Bandwidth~(UWB) based distance measurements module on each of the mobile AR devices which is capable of self-localization with respect to the environment. As users move on independent paths, we collect the positions of the AR devices in their local frames and the corresponding distance measurements. Based on the registration, we support to create a spontaneous collaborative AR environment to spatially coordinate users' interactions. We run both technical evaluation and user studies to investigate the registration accuracy and the usability towards spatial collaborations. Finally, we demonstrate various collaborative AR experience using SynchronizAR.",
        "authors": [
            {
                "name": "Ke  Huo, Purdue University"
            },
            {
                "name": "Tianyi  Wang, Purdue University"
            },
            {
                "name": "Luis  Paredes, Purdue University"
            },
            {
                "name": "Ana M. Villanueva, Purdue University"
            },
            {
                "name": "Yuanzhi  Cao, Purdue University"
            },
            {
                "name": "Karthik  Ramani, Purdue University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1047.jpg",
        "keywords": [
            "Augmented Reality",
            "Collaboration",
            "Spatial Interactions",
            "Registration"
        ],
        "subtype": "paper",
        "title": "SynchronizAR: Instant Synchronization for Spontaneous and Spatial Collaborations in Augmented Reality",
        "type": "paper"
    },
    "ufp1056": {
        "abstract": "In this paper, we define the concept of exploratory labeling: the use of computational and interactive methods to help analysts categorize groups of documents into a set of unknown and evolving labels. While many computational methods exist to analyze data and build models once the data is organized around a set of predefined categories or labels, few methods address the problem of reliably discovering and curating such labels in the first place. In order to move first steps towards bridging this gap, we propose an interactive visual data analysis method that integrates human-driven label ideation, specification and refinement with machine-driven recommendations. The proposed method enables the user to progressively discover and ideate labels in an exploratory fashion and specify rules that can be used to automatically match sets of documents to labels. To support this process of ideation, specification, as well as evaluation of the labels, we use unsupervised machine learning methods that provide suggestions and data summaries. We evaluate our method by applying it to a real-world labeling problem as well as through controlled user studies to identify and reflect on patterns of interaction emerging from exploratory labeling activities.",
        "authors": [
            {
                "name": "Cristian  Felix, New York University"
            },
            {
                "name": "Aritra  Dasgupta, Pacific Northwest National Lab"
            },
            {
                "name": "Enrico  Bertini, New York University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Exploratory Labeling",
            "Text Analysis",
            "Visualization",
            "Document Labeling"
        ],
        "subtype": "paper",
        "title": "The Exploratory Labeling Assistant: Mixed-Initiative Label Curation with Large Document Collections",
        "type": "paper"
    },
    "ufp1069": {
        "abstract": "Virtual Reality enables users to explore content whose physics are only limited by our creativity. Such limitless environments provide us with many opportunities to explore innovative ways to support productivity and collaboration. We present Spacetime, a scene editing tool built from the ground up to explore the novel interaction techniques that empower single user interaction while maintaining fluid multi-user collaboration in immersive virtual environment. We achieve this by introducing three novel interaction concepts: the Container, a new interaction primitive that supports a rich set of object manipulation and environmental navigation techniques, Parallel Objects, which enables parallel manipulation of objects to resolve interaction conflicts and support design workflows, and Avatar Objects, which supports interaction among multiple users while maintaining an individual users’ agency. Evaluated by professional Virtual Reality designers, Spacetime supports powerful individual and fluid collaborative workflows.",
        "authors": [
            {
                "name": "Haijun  Xia, University of Toronto"
            },
            {
                "name": "Sebastian  Herscher, New York University"
            },
            {
                "name": "Ken  Perlin, New York University"
            },
            {
                "name": "Daniel  Wigdor, University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1069.jpg",
        "keywords": [
            "Virtual Reality",
            "Computer-Supported Collaborative Work",
            "Interaction Techniques"
        ],
        "subtype": "paper",
        "title": "Spacetime: Enabling Fluid Individual and Collaborative Editing in Virtual Reality",
        "type": "paper"
    },
    "ufp1071": {
        "abstract": "While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory.\n\nTo facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.",
        "authors": [
            {
                "name": "Jonathan  Bragg, University of Washington"
            },
            {
                "name": "Mausam  Mausam, Indian Institute of Technology, Delhi"
            },
            {
                "name": "Daniel S. Weld, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1071.jpg",
        "keywords": [
            ",Crowdsourcing",
            "workflow",
            "task design",
            "debugging."
        ],
        "subtype": "paper",
        "title": "Sprout: Crowd-Powered Task Design for Crowdsourcing",
        "type": "paper"
    },
    "ufp1075": {
        "abstract": "We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech. Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize the issued commands with an end-to-end deep learning model. Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps). We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efficiency, and comparing with voiced commands with regards to personal privacy and social norms. We demonstrate that Lip-Interact can help users access functionality efficiently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more fluent.",
        "authors": [
            {
                "name": "Ke  Sun, Tsinghua University"
            },
            {
                "name": "Chun  Yu, Tsinghua University"
            },
            {
                "name": "Weinan  Shi, Tsinghua University"
            },
            {
                "name": "Lan  Liu, Tsinghua University"
            },
            {
                "name": "Yuanchun  Shi, Tsinghua University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1075.jpg",
        "keywords": [
            "Lip interaction",
            "silent speech",
            "mobile interaction",
            "touch-free",
            "semantic gesture",
            "vision-based recognition"
        ],
        "subtype": "paper",
        "title": "Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands",
        "type": "paper"
    },
    "ufp1091": {
        "abstract": "The web has matured as a publishing platform: news outlets regularly publish rich, interactive stories while technical writers use animation and interaction to communicate complex ideas. This style of interactive media has the potential to engage a large audience and more clearly explain concepts, but is expensive and time consuming to produce. Drawing on industry experience and interviews with domain experts, we contribute design tools to make it easier to author and publish interactive articles. We introduce Idyll, a novel \"compile-to-the-web\"language for web-based interactive narratives. Idyll implements a flexible article model, allowing authors control over document style and layout, reader-driven events (such as button clicks and scroll triggers), and a structured interface to JavaScript components. Through both examples and first-use results from undergraduate computer science students, we show how Idyll reduces the amount of effort and custom code required to create interactive articles.\n",
        "authors": [
            {
                "name": "Matthew  Conlen, University of Washington"
            },
            {
                "name": "Jeffrey  Heer, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Artifact or System",
            "Prototyping/Implementation",
            "InteractionDesign",
            "Programming/Development Support",
            "Storytelling",
            "Visualization",
            "Programming Languages"
        ],
        "subtype": "paper",
        "title": "Idyll: A Markup Language for Authoring and Publishing Interactive Articles on the Web",
        "type": "paper"
    },
    "ufp1094": {
        "abstract": "Teleportation is a popular locomotion technique that lets users safely navigate beyond the confines of available positional tracking space without inducing VR sickness. \nBecause available walking space is limited and teleportation is faster than walking, a risk with using teleportation is that users might end up abandoning walking input and only relying on teleportation, which is considered detrimental to presence. We present redirected teleportation; an improved version of teleportation that uses iterative non-obtrusive reorientation and repositioning using a portal to redirect the user back to the center of the tracking space, where available walking space is larger. A user study compares the effectiveness, accuracy, and usability of redirected teleportation with regular teleportation using a navigation task in three different environments.  Results show that redirected teleportation allows for a better utilization of available tracking space than regular teleportation, as it requires significantly fewer teleportations, while users walk more and use a larger portion of the available tracking space. ",
        "authors": [
            {
                "name": "James  Liu, University of Nevada"
            },
            {
                "name": "Hirav  Parekh, University of Nevada Reno"
            },
            {
                "name": "Majed  Al-Zayer, University of Nevada"
            },
            {
                "name": "Eelke  Folmer, University of Nevada"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual reality",
            "locomotion",
            "teleportation",
            "positional tracking"
        ],
        "subtype": "paper",
        "title": "Increasing Walking in VR using Redirected Teleportation",
        "type": "paper"
    },
    "ufp1103": {
        "abstract": "Learning a new software application can be a challenge, requiring the user to enter a new environment where their existing knowledge and skills do not apply, or worse, work against them. To ease this transition, we propose the idea of cross-application bridges that start with the interface of a familiar application, and gradually change their interaction model, tools, conventions, and appearance to resemble that of an application to be learned. To investigate this idea, we developed Blocks-to-CAD, a cross-application bridge from Minecraft-style games to 3D solid modeling. A user study of our system demonstrated that our modifications to the game did not hurt enjoyment or increase cognitive load, and that players could successfully apply knowledge and skills learned in the game to tasks in a popular 3D solid modeling application. The process of developing Blocks-to-CAD also revealed eight design strategies that can be applied to design cross-application bridges for other applications and domains.",
        "authors": [
            {
                "name": "Ben  Lafreniere, Autodesk Research"
            },
            {
                "name": "Tovi  Grossman, Autodesk Research & University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1103.jpg",
        "keywords": [
            "feature-rich software",
            "software learning",
            "skill transfer"
        ],
        "subtype": "paper",
        "title": "Blocks-to-CAD : A Cross-Application Bridge from Minecraft to 3D Modeling",
        "type": "paper"
    },
    "ufp1109": {
        "abstract": "SilentVoice is a new voice input interface device that penetrates the speech-based natural user interface (NUI) in daily life. The proposed \"ingressive speech\"method enables placement of a microphone very close to the front of the mouth without suffering from pop-noise, capturing very soft speech sounds with a good S/N ratio. It realizes ultra-small (less than 39dB(A)) voice leakage, allowing us to use voice input without annoying surrounding people in public and mobile situations as well as offices and homes. By measuring airflow direction, SilentVoice can easily be separated from normal utterances with 98.8% accuracy; no activation words are needed. It can be used for voice-activated systems with a specially trained voice recognizer; evaluation results yield word error rates (WERs) of 1.8% (speaker-dependent condition), and 7.0% (speaker-independent condition) with a limited dictionary of 85 command sentences. A whisper-like natural voice can also be used for real-time voice communication.",
        "authors": [
            {
                "name": "Masaaki  Fukumoto, Microsoft Research"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1109.jpg",
        "keywords": [
            "silent voice input",
            "silent speech input",
            "ingressive speech",
            "unnoticeable",
            "awareLESS",
            "SilentVoice",
            "wearable interface",
            "interface devices",
            "wearables",
            "SilentPhone"
        ],
        "subtype": "paper",
        "title": "SilentVoice: Unnoticeable Voice Input by Ingressive Speech",
        "type": "paper"
    },
    "ufp1110": {
        "abstract": "We present a new haptic device that enables blind users to continuously interact with spatial virtual environments that contain moving objects, as is the case in sports or shooter games. Users interact with DualPanto by operating the me handle with one hand and by holding on to the it handle with the other hand. Each handle is connected to a pantograph haptic input/output device. The key feature is that the two handles are spatially registered with respect to each other. When guiding their avatar through a virtual world using the me handle, spatial registration enables users to track moving objects by having the device guide the output hand. This allows blind players of a 1-on-1 soccer game to race for the ball or evade an opponent; it allows blind players of a shooter game to aim at an opponent and dodge shots. In our user study, blind participants reported very high enjoyment when using the device to play (6.5/7).",
        "authors": [
            {
                "name": "Oliver  Schneider, Hasso Plattner Institute & University of Waterloo"
            },
            {
                "name": "Jotaro  Shigeyama, Hasso Plattner Institute"
            },
            {
                "name": "Robert  Kovacs, Hasso Plattner Institute"
            },
            {
                "name": "Thijs Jan Roumen, Hasso Plattner Institute"
            },
            {
                "name": "Sebastian  Marwecki, Hasso Plattner Institute"
            },
            {
                "name": "Nico  Boeckhoff, Hasso Plattner Institute"
            },
            {
                "name": "Daniel Amadeus  Gloeckner, Hasso Plattner Institute"
            },
            {
                "name": "Jonas  Bounama, Hasso Plattner Institute"
            },
            {
                "name": "Patrick  Baudisch, Hasso Plattner Institute"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1110.jpg",
        "keywords": [
            "haptics",
            "force-feedback",
            "accessibility",
            "blind",
            "visually impaired",
            "gaming"
        ],
        "subtype": "paper",
        "title": "DualPanto: A Haptic Device that Enables Blind Users to Continuously Interact with Virtual Worlds",
        "type": "paper"
    },
    "ufp1112": {
        "abstract": "Programming by Demonstration (PBD) promises to enable data scientists to collect web data.  However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks.  The missing piece is the capability to collect hierarchically-structured data from across many different webpages.  We present Rousillon, a programming system for writing complex web automation scripts by demonstration.  Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows.  To offer this new demonstration model, we developed novel relation selection and generalization algorithms.   In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming.",
        "authors": [
            {
                "name": "Sarah E. Chasins, University of California, Berkeley"
            },
            {
                "name": "Maria  Mueller, University of Washington"
            },
            {
                "name": "Rastislav  Bodik, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1112.jpg",
        "keywords": [
            "Programming by Demonstration",
            "End-User Programming",
            "Web Scraping",
            "Program Synthesis",
            "Generalization",
            "Data Science"
        ],
        "subtype": "paper",
        "title": "Rousillon: Scraping Distributed Hierarchical Web Data",
        "type": "paper"
    },
    "ufp1128": {
        "abstract": "Smartphones are the most successful mobile devices and offer intuitive interaction through touchscreens. Current devices treat all fingers equally and only sense touch contacts on the front of the device. In this paper, we present InfiniTouch, the first system that enables touch input on the whole device surface and identifies the fingers touching the device without external sensors while keeping the form factor of a standard smartphone. We first developed a prototype with capacitive sensors on the front, the back and on three sides. We then conducted a study to train a convolutional neural network that identifies fingers with an accuracy of 95.78% while estimating their position with a mean absolute error of 0.74cm. We demonstrate the usefulness of multiple use cases made possible with InfiniTouch, including finger-aware gestures and finger flexion state as an action modifier.",
        "authors": [
            {
                "name": "Huy Viet  Le, University of Stuttgart"
            },
            {
                "name": "Sven  Mayer, University of Stuttgart"
            },
            {
                "name": "Niels  Henze, University of Stuttgart & University of Regensburg"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1128.jpg",
        "keywords": [
            "touchscreen",
            "machine learning",
            "finger-aware interaction"
        ],
        "subtype": "paper",
        "title": "<i>InfiniTouch:</i> Finger-Aware Interaction on Fully Touch Sensitive Smartphones",
        "type": "paper"
    },
    "ufp1129": {
        "abstract": "Instructors of 3D design workshops for children face many challenges, including maintaining awareness of students’ progress, helping students who need additional attention, and creating a fun experience while still achieving learning goals. To help address these challenges, we developed Maestro, a workshop orchestration system that visualizes students’ progress, automatically detects and draws attention to common challenges faced by students, and provides mechanisms to address common student challenges as they occur. We present the design of Maestro, and the results of a case-study evaluation with an experienced facilitator and 13 children. The facilitator appreciated Maestro’s real-time indications of which students were successfully following her tutorial demonstration, and recognized the system’s potential to “extend her reach” while helping struggling students. Participant interaction data from the study provided support for our follow-along detection algorithm, and the capability to remind students to use 3D navigation.",
        "authors": [
            {
                "name": "Volodymyr  Dziubak, University of Manitoba & Autodesk Research"
            },
            {
                "name": "Ben  Lafreniere, Autodesk Research"
            },
            {
                "name": "Tovi  Grossman, Autodesk Research & University of Toronto"
            },
            {
                "name": "Andrea  Bunt, University of Manitoba"
            },
            {
                "name": "George  Fitzmaurice, Autodesk Research"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "software learning",
            "real-time teaching assistance",
            "analytics"
        ],
        "subtype": "paper",
        "title": "Maestro: Designing a System for Real-Time Orchestration of 3D Modeling Workshops",
        "type": "paper"
    },
    "ufp1133": {
        "abstract": "We present TrussFormer, an integrated end-to-end system that allows users to 3D print large-scale kinetic structures, i.e., structures that involve motion and deal with dynamic forces. TrussFormer builds on TrussFab, from which it inherits the ability to create static large-scale truss structures from 3D printed connectors and PET bottles. TrussFormer adds movement to these structures by placing linear actuators into them: either manually, wrapped in reusable components called assets, or by demonstrating the intended movement. TrussFormer verifies that the resulting structure is mechanically sound and will withstand the dynamic forces resulting from the motion. To fabricate the design, TrussFormer generates the underlying hinge system that can be printed on standard desktop 3D printers.  We demonstrate TrussFormer with several example objects, including a 6 legged walking robot and a 4m tall animatronics dinosaur with 5 degrees of freedom.",
        "authors": [
            {
                "name": "Robert  Kovacs, Hasso Plattner Institute"
            },
            {
                "name": "Alexandra  Ion, Hasso Plattner Institute"
            },
            {
                "name": "Pedro  Lopes, Hasso Plattner Institute"
            },
            {
                "name": "Tim  Oesterreich, Hasso Plattner Institute"
            },
            {
                "name": "Johannes  Filter, Hasso Plattner Institute"
            },
            {
                "name": "Philipp  Otto, Hasso Plattner Institute"
            },
            {
                "name": "Tobias  Arndt, Hasso Plattner Institute"
            },
            {
                "name": "Nico  Ring, Hasso Plattner Institute"
            },
            {
                "name": "Melvin  Witte, Hasso Plattner Institute"
            },
            {
                "name": "Anton  Synytsia, Oregon State University"
            },
            {
                "name": "Patrick  Baudisch, Hasso Plattner Institute"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1133.jpg",
        "keywords": [
            "Fabrication",
            "3D printing",
            "variable geometry truss",
            "large scale mechanism"
        ],
        "subtype": "paper",
        "title": "TrussFormer: 3D Printing Large Kinetic Structures",
        "type": "paper"
    },
    "ufp1138": {
        "abstract": "Smart and responsive environments rely on the ability to detect physical events, such as appliance use and human activities. Currently, to sense these types of events, one must either upgrade to “smart” appliances, or attach aftermarket sensors to existing objects. These approaches can be expensive, intrusive and inflexible. In this work, we present Vibrosight, a new approach to sense activities across entire rooms using long-range laser vibrometry. Unlike a microphone, our approach can sense physical vibrations at one specific point, making it robust to interference from other activities and noisy environments. This property enables detection of simultaneous activities, which has proven challenging in prior work. Through a series of evaluations, we show that Vibrosight can offer high accuracies at long range, allowing our sensor to be placed in an inconspicuous location. We also explore a range of additional uses, including data transmission, sensing user input and modes of appliance operation, and detecting human movement and activities on work surfaces.",
        "authors": [
            {
                "name": "Yang  Zhang, Carnegie Mellon University"
            },
            {
                "name": "Gierad  Laput, Carnegie Mellon University"
            },
            {
                "name": "Chris  Harrison, Carnegie Mellon University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1138.jpg",
        "keywords": [
            "Context-Aware Sensing",
            "Internet-of-Things",
            "Appliance Monitoring",
            "Activity Detection",
            "Laser Vibrometry."
        ],
        "subtype": "paper",
        "title": "Vibrosight: Long-Range Vibrometry for Smart Environment Sensing",
        "type": "paper"
    },
    "ufp1140": {
        "abstract": "Despite sound being a rich source of information, computing devices with microphones do not leverage audio to glean useful insights about their physical and social context. For example, a smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a user is doing in a kitchen – a missed opportunity. In this work, we describe a novel, real-time, sound-based activity recognition system. We start by taking an existing, state-of-the-art sound labeling model, which we then tune to classes of interest by drawing data from professional sound effect libraries traditionally used in the entertainment industry. These well-labeled and high-quality sounds are the perfect atomic unit for data augmentation, including amplitude, reverb, and mixing, allowing us to exponentially grow our tuning data in realistic ways. We quantify the performance of our approach across a range of environments and device categories and show that microphone-equipped computing devices already have the requisite capability to unlock real-time activity recognition comparable to human accuracy.",
        "authors": [
            {
                "name": "Gierad  Laput, Carnegie Mellon University"
            },
            {
                "name": "Karan  Ahuja, Carnegie Mellon University"
            },
            {
                "name": "Mayank  Goel, Carnegie Mellon University"
            },
            {
                "name": "Chris  Harrison, Carnegie Mellon University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1140.jpg",
        "keywords": [
            "Ubiquitous sensing",
            "Internet-of-Things",
            "IoT",
            "Smart Environments",
            "Acoustics",
            "Sound Sensing",
            "Microphones"
        ],
        "subtype": "paper",
        "title": "Ubicoustics: Plug-and-Play Acoustic Activity Recognition",
        "type": "paper"
    },
    "ufp1147": {
        "abstract": "Portable electronic navigation systems are often used for directional guidance when humans need to navigate terrain quickly and accurately. Prior work in this field has focused on using either the visual or haptic sensory modality for providing such guidance, and results have indicated that either option may be preferable depending upon the user's specific needs. However, conventional methods involve selecting a single modality based on which will work best with the task the user is most likely to perform and using this modality throughout the duration of the navigation. In this paper, we describe the design and results of a study intended to evaluate the effectiveness of an adaptive modality selection algorithm that dynamically selects a navigation system's directional guidance modality while considering both task-specific benefits and the time-varying effects of switching cost, stimulus-specific adaptation, and habituation. Our findings indicate that use of this algorithm can improve user performance in the presence of multiple simultaneous tasks.",
        "authors": [
            {
                "name": "Kyle  Kotowick, Massachusetts Institute of Technology"
            },
            {
                "name": "Julie  Shah, Massachusetts Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Multimodal",
            "Sensory Modality",
            "Navigation"
        ],
        "subtype": "paper",
        "title": "Intelligent Modality Selection for Navigation Systems",
        "type": "paper"
    },
    "ufp1148": {
        "abstract": "Mobile devices offer people the opportunity to get useful tasks done during time previously thought to be unusable. Because mobile devices have small screens and are often used in divided attention scenarios, people are limited to using them for short, simple tasks; complex tasks like editing a document present significant challenges in this environment. In this paper we demonstrate how a complex task requiring focused attention can be adapted to the fragmented way people work while mobile by decomposing the task into smaller, simpler microtasks. We introduce Play Write, a microproductivity tool that allows people to edit Word documents from their phones via such microtasks. When participants used Play Write while simultaneously watching a video, we found that they strongly preferred its microtask-based editing approach to the traditional editing experience offered by Mobile Word. Play Write made participants feel more productive and less stressed, and they completed more edits with it. Our findings suggest microproductivity tools like Play Write can help people be productive in divided attention scenarios.",
        "authors": [
            {
                "name": "Shamsi T. Iqbal, Microsoft Research"
            },
            {
                "name": "Jaime  Teevan, Microsoft Research"
            },
            {
                "name": "Dan  Liebling, Microsoft Research & Google"
            },
            {
                "name": "Anne Loomis Thompson, Microsoft Research"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1148.jpg",
        "keywords": [
            "Interruptions",
            "microtasking",
            "multitasking",
            "microproductivity"
        ],
        "subtype": "paper",
        "title": "Multitasking with Play Write, a Mobile Microproductivity Writing Tool",
        "type": "paper"
    },
    "ufp1150": {
        "abstract": "We present an interactive tool to animate the visual elements of a static picture, based on simple sketch-based markup. While animated images enhance websites, infographics, logos, e-books, and social media, creating such animations from still pictures is difficult for novices and tedious for experts. Creating automatic tools is challenging due to ambiguities in object segmentation, relative depth ordering, and non-existent temporal information. With a few user drawn scribbles as input, our mixed initiative creative interface extracts repetitive texture elements in an image, and supports animating them. Our system also facilitates the creation of multiple layers to enhance depth cues in the animation. Finally, after analyzing the artwork during segmentation, several animation processes automatically generate kinetic textures that are spatio-temporally coherent with the source image. Our results, as well as feedback from our user evaluation, suggest that our system effectively allows illustrators and animators to add life to still images in a broad range of visual styles.",
        "authors": [
            {
                "name": "Nora S. Willett, Princeton University"
            },
            {
                "name": "Rubaiat Habib  Kazi, Autodesk"
            },
            {
                "name": "Michael  Chen, Autodesk"
            },
            {
                "name": "George  Fitzmaurice, Autodesk Research"
            },
            {
                "name": "Adam  Finkelstein, Princeton University"
            },
            {
                "name": "Tovi  Grossman, Autodesk Research & University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1150.jpg",
        "keywords": [
            "Kinetic textures",
            "animation",
            "dynamics",
            "pictures"
        ],
        "subtype": "paper",
        "title": "A Mixed-Initiative Interface for Animating Static Pictures",
        "type": "paper"
    },
    "ufp1171": {
        "abstract": "Video prototypes help capture and communicate interaction with paper prototypes in the early stages of design.\nHowever, designers sometimes find it tedious to create stop-motion videos for continuous interactions and to re-shoot clips as the design evolves.\nWe introduce Montage, a proof-of-concept implementation of a computer-assisted process for video prototyping.\nMontage lets designers progressively augment video prototypes with digital sketches, facilitating the creation, reuse and exploration of dynamic interactions.\nMontage uses chroma keying to decouple the prototyped interface from its context of use, letting designers reuse or change them independently.\nWe describe how Montage enhances video prototyping by combining video with digital animated sketches, encourages the exploration of different contexts of use, and supports prototyping of different interaction styles.",
        "authors": [
            {
                "name": "Germán  Leiva, Université Paris-Sud, CNRS, Inria, Université Paris-Saclay"
            },
            {
                "name": "Michel  Beaudouin-Lafon, Université Paris-Sud, CNRS, Inria, Université Paris-Saclay"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1171.jpg",
        "keywords": [
            "Video prototyping",
            "Paper prototyping",
            "Wizard-of-Oz"
        ],
        "subtype": "paper",
        "title": "Montage: A Video Prototyping System to Reduce Re-Shooting and Increase Re-Usability",
        "type": "paper"
    },
    "ufp1179": {
        "abstract": "Smartphones are now a central technology in the daily lives of billions, but it relies on its battery to perform. Battery optimization is thereby a crucial design constraint in any mobile OS and device. However, even with new low-power methods, the ever-growing touchscreen remains the most power-hungry component. We propose an Ultra-Low-Power Mode (ULPM) for mobile devices that allows for touch interaction without visual feedback and exhibits significant power savings of up to 60% while allowing to complete interactive tasks. We demonstrate the effectiveness of the screenless ULPM in text-entry tasks, camera usage, and listening to videos, showing only a small decrease in usability for typical users.",
        "authors": [
            {
                "name": "Jian  Xu, Stony Brook University"
            },
            {
                "name": "Suwen  Zhu, Stony Brook University"
            },
            {
                "name": "Aruna  Balasubramanian, Stony Brook University"
            },
            {
                "name": "Xiaojun  Bi, Stony Brook University"
            },
            {
                "name": "Roy  Shilkrot, Stony Brook University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Power Saving",
            "Mobile System",
            "Text Entry",
            "Touchscreen"
        ],
        "subtype": "paper",
        "title": "Ultra-Low-Power Mode for Screenless Mobile Interaction",
        "type": "paper"
    },
    "ufp1186": {
        "abstract": "We present magneto-haptics, a design approach of haptic sensations powered by the forces present among permanent magnets during active touch. Magnetic force has not been efficiently explored in haptic design because it is not intuitive and there is a lack of methods to associate or visualize magnetic force with haptic sensations, especially for complex magnetic patterns. To represent the haptic sensations of magnetic force intuitively, magneto-haptics formularizes haptic potential from the distribution of magnetic force along the path of motion. It provides a rapid way to compute the relationship between the magnetic phenomena and the haptic mechanism. Thus, we can convert a magnetic force distribution into a haptic sensation model, making the design of magnet-embedded haptic sensations more efficient. We demonstrate three applications of magneto-haptics through interactive interfaces and devices. We further verify our theory by evaluating some magneto-haptic designs through experiments.",
        "authors": [
            {
                "name": "Masa  Ogata, National Institute of Advanced Industrial Science and Technology (AIST)"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1186.jpg",
        "keywords": [
            "Magneto-haptics",
            "magnetism",
            "haptic feedback",
            "permanent magnet",
            "physical interaction"
        ],
        "subtype": "paper",
        "title": "Magneto-Haptics: Embedding Magnetic Force Feedback for Physical Interactions",
        "type": "paper"
    },
    "ufp1187": {
        "abstract": "Accessibility issues in mobile apps make those apps difficult or impossible to access for many people. Examples include elements that fail to provide alternative text for a screen reader, navigation orders that are difficult, or custom widgets that leave key functionality inaccessible. Social annotation techniques have demonstrated compelling approaches to such accessibility concerns in the web, but have been difficult to apply in mobile apps because of the challenges of robustly annotating interfaces. This research develops methods for robust annotation of mobile app interface elements. Designed for use in runtime interface modification, our methods are based in screen identifiers, element identifiers, and screen equivalence heuristics. We implement initial developer tools for annotating mobile app accessibility metadata, evaluate our current screen equivalence heuristics in a dataset of 2038 screens collected from 50 mobile apps, present three case studies implementing runtime repair of common accessibility issues, and examine repair of real-world accessibility issues in 26 apps. These contributions overall demonstrate strong opportunities for social annotation in mobile accessibility.",
        "authors": [
            {
                "name": "Xiaoyi  Zhang, University of Washington"
            },
            {
                "name": "Anne Spencer Ross, University of Washington"
            },
            {
                "name": "James  Fogarty, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1187.jpg",
        "keywords": [
            "Robust annotation",
            "runtime modification",
            "accessibility"
        ],
        "subtype": "paper",
        "title": "Robust Annotation of Mobile Application Interfaces in Methods for Accessibility Repair and Enhancement",
        "type": "paper"
    },
    "ufp1200": {
        "abstract": "Document authors commonly use tables to support arguments presented in\nthe text. But, because tables are usually separate from the main body\ntext, readers must split their attention between different parts of\nthe document.\nWe present an interactive document reader that automatically links\ndocument text with corresponding table cells.\nReaders can select a\nsentence (or tables cells) and our reader highlights the relevant\ntable cells (or sentences).\nWe provide an automatic pipeline for\nextracting such references between sentence text and table cells for\nexisting PDF documents that combines structural analysis of tables\nwith natural language processing and rule-based matching.\nOn a test corpus of 330 (sentence, table) pairs, our pipeline correctly\nextracts 48.8% of the references. An additional 30.5% contain only\nfalse negatives (FN) errors -- the reference is missing table cells.\nThe remaining 20.7% contain false positives (FP) errors -- the reference\nincludes extraneous table cells and could therefore mislead readers.\nA user study finds that despite such errors, our interactive document\nreader helps readers match sentences with corresponding table\ncells more accurately and quickly than a baseline document reader.",
        "authors": [
            {
                "name": "Dae Hyun  Kim, Stanford University"
            },
            {
                "name": "Enamul  Hoque, Stanford University"
            },
            {
                "name": "Juho  Kim, Korea Advanced Institute of Science and Technology"
            },
            {
                "name": "Maneesh  Agrawala, Stanford University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Interactive documents",
            "Visualization",
            "Text analysis"
        ],
        "subtype": "paper",
        "title": "Facilitating Document Reading by Linking Text and Tables",
        "type": "paper"
    },
    "ufp1215": {
        "abstract": "Performance animation is an expressive method for animating characters through human performance. However, character motion is only one part of creating animated stories. The typical workflow also involves writing a script, coordinating actors, and editing recorded performances. In most cases, these steps are done in isolation with separate tools, which introduces friction and hinders iteration. We propose TakeToons, a script-driven approach that allows authors to annotate standard scripts with relevant animation events like character actions, camera positions, and scene backgrounds. We compile this script into a story model that persists throughout the production process and provides a consistent structure for organizing and assembling recorded performances and propagating script or timing edits to existing recordings. TakeToons enables writing, performing and editing to happen in an integrated and interleaved manner that streamlines production and facilitates iteration. Informal feedback from professional animators suggests that our approach can benefit many existing workflows supporting individual authors and production teams with many different contributors.",
        "authors": [
            {
                "name": "Hariharan  Subramonyam, University of Michigan & Adobe Research"
            },
            {
                "name": "Wilmot  Li, Adobe Research"
            },
            {
                "name": "Eytan  Adar, University of Michigan & Adobe Research"
            },
            {
                "name": "Mira  Dontcheva, Adobe Research"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "performance animation",
            "animation script",
            "speech"
        ],
        "subtype": "paper",
        "title": "TakeToons: Script-driven Performance Animation",
        "type": "paper"
    },
    "ufp1216": {
        "abstract": "Makers often create both physical and digital prototypes to explore a design, taking advantage of the subtle feel of physical materials and the precision and power of digital models. We introduce ShapeMe, a novel smart material that captures its own geometry as it is physically cut by an artist or designer. ShapeMe includes a software toolkit that lets its users generate customized, embeddable sensors that can accommodate various object shapes. As the designer works on a physical prototype, the toolkit streams the artist's physical changes to its digital counterpart in a 3D CAD environment.\nWe use a rapid, inexpensive and simple-to-manufacture inkjet printing technique to create embedded sensors. We successfully created a linear predictive model of the sensors' lengths, and our empirical tests of ShapeMe show an average accuracy of 2 to 3 mm. We present two application scenarios for modeling multi-object constructions, such as architectural models, and 3D models consisting of multiple layers stacked one on top of each other. ShapeMe demonstrates a novel technique for integrating digital and physical modeling, and suggests new possibilities for creating shape-aware materials.",
        "authors": [
            {
                "name": "Michael  Wessely, Inria; Univ. Paris-Sud & CNRS (LRI); Université Paris-Saclay"
            },
            {
                "name": "Theophanis  Tsandilas, Inria; Univ. Paris-Sud & CNRS (LRI); Université Paris-Saclay"
            },
            {
                "name": "Wendy E. Mackay, Inria; Univ. Paris-Sud & CNRS (LRI); Université Paris-Saclay"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1216.jpg",
        "keywords": [
            "Personal Fabrication",
            "Sensing Technologies",
            "Shape-Aware Materials",
            "Printed Electronics",
            "Physical Modeling"
        ],
        "subtype": "paper",
        "title": "Shape-Aware Material: Interactive Fabrication with ShapeMe",
        "type": "paper"
    },
    "ufp1217": {
        "abstract": "This paper presents a technique enabling distributed batteryless near-field identification (ID) between two passive radio frequency ID (RFID) tags. Each conventional ultra-high-frequency (UHF) RFID tag is modified by connecting its antenna and chip to a reed switch and then attaching a magnet to one of the reed switch's terminals, thus transforming it into an always-on switch. When the two modules approach each other, the magnets counteract each other and turn off both switches at the same time. The coabsence of IDs thus indicates a unique interaction event. In addition to sensing, the module also provides native haptic feedback through magnetic repulsion force, enabling users to perceive the system's state eyes-free, without physical constraints. Additional visual feedback can be provided through an energy-harvesting module and a light emitting diode. This specific hardware design supports contactless, orientation-invariant sensing, with a form factor compact enough for embedded and wearable use in ubiquitous computing applications.",
        "authors": [
            {
                "name": "Rong-Hao  Liang, Eindhoven University of Technology"
            },
            {
                "name": "Meng-Ju  Hsieh, National Taiwan University"
            },
            {
                "name": "Jheng-You  Ke, National Taiwan University"
            },
            {
                "name": "Jr-Ling  Guo, National Taiwan University"
            },
            {
                "name": "Bing-Yu  Chen, National Taiwan University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1217.jpg",
        "keywords": [
            "RFID",
            "Batteryless",
            "Magnet",
            "Reed Switch",
            "Near-Field Identification",
            "Wearables"
        ],
        "subtype": "paper",
        "title": "RFIMatch: Distributed Batteryless Near-Field Identification Using RFID-Tagged Magnet-Biased Reed Switches",
        "type": "paper"
    },
    "ufp1218": {
        "abstract": "End-user elicitation studies are a popular design method, but their data require substantial time and effort to analyze. In this paper, we present Crowdsensus, a crowd-powered tool that enables researchers to efficiently analyze the results of elicitation studies using subjective human judgment and automatic clustering algorithms. In addition to our own analysis, we asked six expert researchers with experience running and analyzing elicitation studies to analyze an end-user elicitation dataset of 10 functions for operating a web-browser, each with 43 voice commands elicited from end-users for a total of 430 voice commands. We used Crowdsensus to gather similarity judgments of these same 430 commands from 410 online crowd workers. The crowd outperformed the experts by arriving at the same results for seven of eight functions and resolving a function where the experts failed to agree. Also, using Crowdsensus was about four times faster than using experts. ",
        "authors": [
            {
                "name": "Abdullah X. Ali, University of Washington"
            },
            {
                "name": "Meredith Ringel Morris, Microsoft Research"
            },
            {
                "name": "Jacob O. Wobbrock, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "End-user elicitation study",
            "agreement rate",
            "online crowds",
            "crowdsourcing",
            "human computation",
            "Mechanical Turk"
        ],
        "subtype": "paper",
        "title": "Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies",
        "type": "paper"
    },
    "ufp1226": {
        "abstract": "Novel interactions that capacitively couple electromagnetic (EM) fields between devices and the human body are gaining more attention in the human-computer interaction community. One class of these techniques is Body Channel Communication (BCC), a method that overlays physical touch with digital information. Despite the number of published capacitive sensing and communication prototypes, there exists no guideline on how to design such hardware or what are the application limitations and possibilities. Specifically, wearable (groundless) BCC has been proven in the past to be extremely challenging to implement. Additionally, the exact behavior of the human body as an EM-field medium is still not fully understood today. Consequently, the application domain of BCC technology could not be fully explored.\n\nThis paper addresses this problem. Based on a recently published general purpose wearable BCC system, we first present a thorough evaluation of the impact of various technical parameter choices and an exhaustive channel characterization of the human body as a host for BCC. Second, we discuss the implications of these results for the application design space and present guidelines for future wearable BCC systems and their applications. Third, we point out an important observation of the measurements, namely that BCC can employ the whole body as user interface (and not just hands or feet). We sketch several applications with these novel interaction modalities.",
        "authors": [
            {
                "name": "Virag  Varga, ETH Zurich & Disney Research"
            },
            {
                "name": "Marc  Wyss, ETH Zurich"
            },
            {
                "name": "Gergely  Vakulya, ETH Zurich & Disney Research"
            },
            {
                "name": "Alanson  Sample, Disney Research"
            },
            {
                "name": "Thomas R. Gross, ETH Zurich"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1226.jpg",
        "keywords": [
            "body channel communication",
            "capacitive coupling",
            "channel characterization",
            "empirical study",
            "wearables",
            "guideline",
            "interaction techniques"
        ],
        "subtype": "paper",
        "title": "Designing Groundless Body Channel Communication Systems: Performance and Implications",
        "type": "paper"
    },
    "ufp1230": {
        "abstract": "Automation is one of the key solutions proposed and adopted by international Air Transport research programs to meet the challenges of increasing air traffic. For automation to be safe and usable, it needs to be suitable to the activity it supports, both when authoring it and when operating it. Here we present Vizir, a Domain-Specific Graphical Language and an Environment for authoring and operating airport automations. We used a participatory-design process with Air Traffic Controllers to gather requirements for Vizir and to design its features. Vizir combines visual interaction-oriented programming constructs with activity-related geographic areas and events. Vizir offers explicit human-control constructs, graphical substrates and means to scale-up with multiple automations. We propose a set of guidelines to inspire designers of similar usable hybrid human-automation systems.",
        "authors": [
            {
                "name": "Stéphane  Conversy, ENAC - University of Toulouse"
            },
            {
                "name": "Jeremie  Garcia, ENAC - University of Toulouse"
            },
            {
                "name": "Guilhem  Buisan, ENAC - University of Toulouse"
            },
            {
                "name": "Mathieu  Cousy, ENAC - University of Toulouse"
            },
            {
                "name": "Mathieu  Poirier, ENAC - University of Toulouse"
            },
            {
                "name": "Nicolas  Saporito, ENAC - University of Toulouse"
            },
            {
                "name": "Damiano  Taurino, Deep Blue"
            },
            {
                "name": "Giuseppe  Frau, Deep Blue"
            },
            {
                "name": "Johan  Debattista, Malta Air Services (MATS)"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1230.jpg",
        "keywords": [
            "Automation",
            "Domain-Specific Language",
            "Visual\nProgramming",
            "Air Traffic Control"
        ],
        "subtype": "paper",
        "title": "Vizir: A Domain-Specific Graphical Language for Authoring and Operating Airport Automations",
        "type": "paper"
    },
    "ufp1233": {
        "abstract": "Although severe aircraft accidents have been reduced in the last decades, the number of injuries and fatalities caused by turbulence is still rising. Current aviation weather products are unable to provide a holistic and intuitive view of the overall weather situation, especially in terms of turbulence forecasts. This work introduces an interactive 3D prototype developed with a user-centered design approach. The prototype focuses on the visualization of significant weather charts, which are utilized during flight preparation. An online user study is conducted to compare the prototype with today's 2D paper maps. A total of 64 pilots from an internationally operating airline participated in the study. Among the major findings of the study is that the prototype significantly decreased the cognitive load, and enhanced spatial awareness and usability. To determine the spatial awareness, a novel similarity measure for spatial configurations of aviation weather data is introduced.",
        "authors": [
            {
                "name": "Lisa  Stähli, ETH Zurich"
            },
            {
                "name": "David  Rudi, ETH Zurich"
            },
            {
                "name": "Martin  Raubal, ETH Zurich"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Aviation",
            "Weather",
            "User Interface",
            "3D",
            "Web",
            "Turbulence",
            "Spatial Awareness"
        ],
        "subtype": "paper",
        "title": "Turbulence Ahead - A 3D Web-Based Aviation Weather Visualizer",
        "type": "paper"
    },
    "ufp1234": {
        "abstract": "We present 4DMesh, a method of combining shrinking and bending thermoplastic actuators with customized geometric algorithms to 4D print and morph centimeter- to meter-sized functional non-developable surfaces. We will share two end-to-end inverse design algorithms. With our tools, users can input CAD models of target surfaces and produce respective printable files. The flat sheet printed can morph into target surfaces when triggered by heat. This system saves shipping and packaging costs, in addition to enabling customizability for the design of relatively large non-developable structures. We designed a few functional artifacts to leverage the advantage of non-developable surfaces for their unique functionalities in aesthetics, mechanical strength, geometric ergonomics and other functionalities. In addition, we demonstrated how this technique can potentially be adapted to customize molds for industrial parts (e.g., car, boat, etc.) in the future. ",
        "authors": [
            {
                "name": "Guanyun  Wang, Carnegie Mellon University"
            },
            {
                "name": "Humphrey  Yang, Carnegie Mellon University"
            },
            {
                "name": "Zeyu  Yan, Carnegie Mellon University"
            },
            {
                "name": "Nurcan  Gecer Ulu, Carnegie Mellon University"
            },
            {
                "name": "Ye  Tao, Carnegie Mellon University & Zhejiang University"
            },
            {
                "name": "Jianzhe  Gu, Carnegie Mellon University"
            },
            {
                "name": "Levent Burak  Kara, Carnegie Mellon University"
            },
            {
                "name": "Lining  Yao, Carnegie Mellon University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1234.jpg",
        "keywords": [
            "4D printing",
            "3D printing",
            "shape changing interfaces",
            "non-developable surface",
            "mesh surface",
            "morphing",
            "self-folding",
            "self-assembly"
        ],
        "subtype": "paper",
        "title": "4DMesh: 4D Printing Morphing Non-Developable Mesh Surfaces",
        "type": "paper"
    },
    "ufp1254": {
        "abstract": "In this paper, we explore the interaction space of MobiLimb, a small 5-DOF serial robotic manipulator attached to a mobile device. It (1) overcomes some limitations of mobile devices (static, passive, motionless); (2) preserves their form factor and I/O capabilities; (3) can be easily attached to or removed from the device; (4) offers additional I/O capabilities such as physical deformation and (5) can support various modular elements such as sensors, lights or shells. We illustrate its potential through three classes of applications: As a tool, MobiLimb offers tangible affordances and an expressive controller that can be manipulated to control virtual and physical objects. As a partner, it reacts expressively to users’ actions to foster curiosity and engagement or assist users. As a medium, it provides rich haptic feedback such as strokes, pat and other tactile stimuli on the hand or the wrist to convey emotions during mediated multimodal communications.\n",
        "authors": [
            {
                "name": "Marc  Teyssier, LTCI, Télécom ParisTech, Université Paris-Saclay & Sorbonne Université"
            },
            {
                "name": "Gilles  Bailly, Sorbonne Université, CNRS, ISIR"
            },
            {
                "name": "Catherine  Pelachaud, Sorbonne Université, CNRS, ISIR"
            },
            {
                "name": "Eric  Lecolinet, LTCI, Télécom ParisTech, Université Paris-Saclay"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Mobile device",
            "Actuated device",
            "Robotics",
            "Mobile Augmentation",
            "Robotic limb"
        ],
        "subtype": "paper",
        "title": "MobiLimb: Augmenting Mobile Devices with a Robotic Limb",
        "type": "paper"
    },
    "ufp1261": {
        "abstract": "We propose using a single slip tactile pixel on virtual reality controllers to produce sensations of finger sliding and textures. When a user moves the controller on a virtual surface, we add a slip opposite to the movement, creating an illusion of a finger that is sliding on the surface, while varying the slip feedback changes lateral forces on fingertip. When coupled with hand motion the lateral forces can be used to create perceptions of artificial textures. RollingStone has been implemented as a prototype VR controller consisting of a ball-based slip display positioned under the user’s fingertip. Within the slip display, a pair of motors actuates the ball, which is capable of gener- ating both short- and long-term two-degree-of-freedom slip feedback. An exploratory study was conducted to ensure that changing the relative motion between the finger and the ball could alter the perceptions conveying the properties of a tex- ture. The following two perception-based studies examined the minimum changes in speed of slip and angle of slip that are detectable by users. The results help us to design haptic patterns as well as our prototype applications. Finally, our preliminary user evaluation indicated that participants wel- comed RollingStone as a useful addition to the range of VR controllers.",
        "authors": [
            {
                "name": "Jo-Yu  Lo, National Taiwan University"
            },
            {
                "name": "Da-Yuan  Huang, National Chiao Tung University"
            },
            {
                "name": "Chen-Kuo  Sun, National Taiwan University of Science and Technology"
            },
            {
                "name": "Chu-En  Hou, National Taiwan University of Science and Technology"
            },
            {
                "name": "Bing-Yu  Chen, National Taiwan University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1261.jpg",
        "keywords": [
            "Haptics",
            "Controller Design",
            "Tactile Display",
            "Slip Display",
            "Virtual Reality"
        ],
        "subtype": "paper",
        "title": "RollingStone: Using Single Slip Taxel for Enhancing Active Finger Exploration with a Virtual Reality Controller",
        "type": "paper"
    },
    "ufp1274": {
        "abstract": "The sensation of being able to feel the shape of an object when grasping it in Virtual Reality (VR) enhances a sense of presence and the ease of object manipulation. Though most prior works focus on force feedback on fingers, the haptic emulation of grasping a 3D shape requires the sensation of touch using the entire hand. Hence, we present Pop-up Prop on Palm (PuPoP), a light-weight pneumatic shape-proxy interface worn on the palm that pops several airbags up with predefined primitive shapes for grasping. When a user's hand encounters a virtual object, an airbag of appropriate shape, ready for grasping, is inflated by way of the use of air pumps; the airbag then deflates when the object is no longer in play. Since PuPoP is a physical prop, it can provide the full sensation of touch to enhance the sense of realism for VR object manipulation. For this paper, we first explored the design and implementation of PuPoP with multiple shape structures. We then conducted two user studies to further understand its applicability. The first study shows that, when in conflict, visual sensation tends to dominate over touch sensation, allowing a prop with a fixed size to represent multiple virtual objects with similar sizes. The second study compares PuPoP with controllers and free-hand manipulation in two VR applications. The results suggest that utilization of dynamically-changing PuPoP, when grasped by users in line with the shapes of virtual objects, enhances enjoyment and realism. We believe that PuPoP is a simple yet effective way to convey haptic shapes in VR.",
        "authors": [
            {
                "name": "Shan-Yuan  Teng, National Taiwan University"
            },
            {
                "name": "Tzu-Sheng  Kuo, National Taiwan University"
            },
            {
                "name": "Chi  Wang, National Taiwan University of Science and Technology"
            },
            {
                "name": "Chi-Huan  Chiang, National Taiwan University"
            },
            {
                "name": "Da-Yuan  Huang, National Chiao Tung University"
            },
            {
                "name": "Liwei  Chan, National Chiao Tung University"
            },
            {
                "name": "Bing-Yu  Chen, National Taiwan University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1274.jpg",
        "keywords": [
            "Haptics",
            "Virtual Reality",
            "Airbag",
            "Shape-Proxy"
        ],
        "subtype": "paper",
        "title": "PuPoP: Pop-up Prop on Palm for Virtual Reality",
        "type": "paper"
    },
    "ufp1275": {
        "abstract": "In this paper, we propose using the auricle – the visible part of the ear – as a means of expressive output to extend body language to convey emotional states. With an initial exploratory study, we provide an initial set of dynamic and static auricular postures. Using these results, we examined the relationship between emotions and auricular postures, noting that dynamic postures involving stretching the top helix in fast (e.g., 2Hz) and slow speeds (1Hz) conveyed intense and mild pleasantness while static postures involving bending the side or top helix towards the center of the ear were associated with intense and mild unpleasantness. Based on the results, we developed a prototype (called Orrechio) with miniature motors, custom-made robotic arms and other electronic components. A preliminary user evaluation showed that participants feel more comfortable using expressive auricular postures with people they are familiar with, and that it is a welcome addition to the vocabulary of human body language.",
        "authors": [
            {
                "name": "Da-Yuan  Huang, Dartmouth College & National Chiao Tung University"
            },
            {
                "name": "Teddy  Seyed, University of Calgary"
            },
            {
                "name": "Linjun  Li, Dartmouth College"
            },
            {
                "name": "Jun  Gong, Dartmouth College"
            },
            {
                "name": "Zhihao  Yao, Dartmouth College & Tsinghua University"
            },
            {
                "name": "Yuchen  Jiao, Dartmouth College & Tsinghua University"
            },
            {
                "name": "Xiang 'Anthony' Chen, University of California, Los Angeles"
            },
            {
                "name": "Xing-Dong  Yang, Dartmouth College"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1275.jpg",
        "keywords": [
            "Actuating human body",
            "wearable earpiece",
            "auricle",
            "body language",
            "emotion"
        ],
        "subtype": "paper",
        "title": "Orecchio: Extending Body-Language through Actuated Static and Dynamic Auricular Postures",
        "type": "paper"
    },
    "ufp1276": {
        "abstract": "Currently, “walkable” virtual reality (VR) is achieved by dedicating a room-sized space for VR activities, which is not shared with non-HMD users engaged in their own activities. To achieve the goal of allowing shared use of space for all users while overcoming the obvious difficulty of integrating use with those immersed in a VR experience, we present ShareSpace, a system that allows external users to communicate their needs for physical space to those wearing an HMD and immersed in their VR experience. ShareSpace works by allowing external users to place “shields” in the virtual environment by using a set of physical shield tools. A pad visualizer helps this process by allowing external users to examine the arrangement of virtual shields. We also discuss interaction techniques that minimize the interference between the respective activities of the HMD wearers and the other users of the same physical space. To evaluate our design, a user study was conducted to collect user feedback from participants in four trial scenarios. The results indicate that our ShareSpace system allows users to perform their respective activities with improved engagement and safety. In addition, this study shows that while the HMD users did perceive a considerable degree of interference due to the internal visual indications from the ShareSpace system, they were still more engaged in their VR experience than when interrupted by direct external physical interference initiated by external users.\n",
        "authors": [
            {
                "name": "Keng-Ta  Yang, National Chiao Tung University"
            },
            {
                "name": "Chiu-Hsuan  Wang, Nation Chiao Tung University"
            },
            {
                "name": "Liwei  Chan, National Chiao Tung University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1276.jpg",
        "keywords": [
            "Virtual reality",
            "Shared use of physical space",
            "External users experience"
        ],
        "subtype": "paper",
        "title": "ShareSpace: Facilitating Shared Use of the Physical Space by both VR Head-Mounted Display and External Users",
        "type": "paper"
    },
    "ufp1305": {
        "abstract": "We present a wall-based space manipulation (WSM) technique that enables users to efficiently select and move distant objects by dynamically squeezing their surrounding space in augmented reality. Users can bring a target object closer by dragging a solid plane behind the object and squeezing the space between them and the plane so that they can select and move the object more delicately and efficiently. We furthermore discuss the unique design challenges of WSM, including the dimension of space reduction and the recognition of the reduced space in relation to the real space. We conducted a user evaluation to verify how WSM improves the performance of the hand-centered object manipulation technique on the HoloLens for moving near objects far away and vice versa. The results indicate that WSM overall performed consistently well and significantly improved efficiency while alleviating arm fatigue.",
        "authors": [
            {
                "name": "Han Joo  Chae, Seoul National University"
            },
            {
                "name": "Jeong-In  Hwang, Seoul National University"
            },
            {
                "name": "Jinwook  Seo, Seoul National University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1305.jpg",
        "keywords": [
            "3-D interaction technique",
            "space manipulation",
            "distant object placement",
            "augmented reality"
        ],
        "subtype": "paper",
        "title": "Wall-based Space Manipulation Technique for Efficient Placement of Distant Objects in Augmented Reality",
        "type": "paper"
    },
    "ufp1309": {
        "abstract": "Modern web development is rife with complexity at all layers, ranging from needing to configure backend services to grappling with frontend frameworks and dependencies. To lower these development barriers, we introduce a technique that enables people to prototype opportunistically by borrowing pieces of desired functionality from across the web without needing any access to their underlying codebases, build environments, or server backends. We implemented this technique in a browser extension called Fusion, which lets users create web UI mashups by extracting components from existing unmodified webpages and hooking them together using transclusion and JavaScript glue code. We demonstrate the generality and versatility of Fusion via a case study where we used it to create seven UI mashups in domains such as programming tools, data science, web design, and collaborative work. Our mashups include replicating portions of prior HCI systems (Blueprint for in-situ code search and DS.js for in-browser data science), extending the p5.js IDE for Processing with real-time collaborative editing, and integrating Python Tutor code visualizations into static tutorials. These UI mashups each took less than 15 lines of JavaScript glue code to create with Fusion.\n",
        "authors": [
            {
                "name": "Xiong  Zhang, University of Rochester"
            },
            {
                "name": "Philip J. Guo, University of California, San Diego"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "opportunistic programming",
            "UI mashups",
            "web prototyping"
        ],
        "subtype": "paper",
        "title": "Fusion: Opportunistic Web Prototyping with UI Mashups",
        "type": "paper"
    },
    "ufp1312": {
        "abstract": "It can be hard for tutorial creators to get fine-grained feedback about how learners are actually stepping through their tutorials and which parts lead to the most struggle. To provide such feedback for technical software tutorials, we introduce the idea of tutorial profiling, which is inspired by software code profiling. We prototyped this idea in a system called Porta that automatically tracks how users navigate through a tutorial webpage and what actions they take on their computer such as running shell commands, invoking compilers, and logging into remote servers. Porta surfaces this trace data in the form of profiling visualizations that augment the tutorial with heatmaps of activity hotspots and markers that expand to show event details, error messages, and embedded screencast videos of user actions. We found through a user study of 3 tutorial creators and 12 students who followed their tutorials that Porta enabled both the tutorial creators and the students to provide more specific, targeted, and actionable feedback about how to improve these tutorials. Porta opens up possibilities for performing user testing of technical documentation in a more systematic and scalable way.",
        "authors": [
            {
                "name": "Alok  Mysore, University of California, San Diego"
            },
            {
                "name": "Philip J. Guo, University of California, San Diego"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "tutorial profiling",
            "activity tracing",
            "software tutorials"
        ],
        "subtype": "paper",
        "title": "Porta: Profiling Software Tutorials Using Operating-System-Wide Activity Tracing",
        "type": "paper"
    },
    "ufp1315": {
        "abstract": "As social agents, robots designed for human interaction must adhere to human social norms. How can we enable designers, engineers, and roboticists to design robot behaviors that adhere to human social norms and do not result in interaction breakdowns? In this paper, we use automated formal-verification methods to facilitate the encoding of appropriate social norms into the interaction design of social robots and the detection of breakdowns and norm violations in order to prevent them. We have developed an authoring environment that utilizes these methods to provide developers of social-robot applications with feedback at design time and evaluated the benefits of their use in reducing such breakdowns and violations in human-robot interactions. Our evaluation with application developers (N=9) shows that the use of formal-verification methods increases designers' ability to identify and contextualize social-norm violations. We discuss the implications of our approach for the future development of tools for effective design of social-robot applications.",
        "authors": [
            {
                "name": "David  Porfirio, University of Wisconsin–Madison"
            },
            {
                "name": "Allison  Sauppé, University of Wisconsin–La Crosse"
            },
            {
                "name": "Aws  Albarghouthi, University of Wisconsin–Madison"
            },
            {
                "name": "Bilge  Mutlu, bilge@cs.wisc.edu"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1315.jpg",
        "keywords": [
            "Human-robot interaction",
            "interaction design",
            "authoring",
            "visual programming",
            "verification",
            "program analysis"
        ],
        "subtype": "paper",
        "title": "Authoring and Verifying Human-Robot Interactions",
        "type": "paper"
    },
    "ufp1317": {
        "abstract": "We present a self-powered module for gesture recognition that utilizes small, low-cost photodiodes for both energy harvesting and gesture sensing. Operating in the photovoltaic mode, photodiodes harvest energy from ambient light. In the meantime, the instantaneously harvested power from individual photodiodes is monitored and exploited as clues for sensing finger gestures in proximity. Harvested power from all photodiodes are aggregated to drive the whole gesture-recognition module including the micro-controller running the recognition algorithm. We design robust, lightweight algorithm to recognize finger gestures in the presence of ambient light fluctuations. We fabricate two prototypes to facilitate user’s interaction with smart glasses and smart watch. Results show 99.7%/98.3% overall precision/recall in recognizing five gestures on glasses and 99.2%/97.5% precision/recall in recognizing seven gestures on the watch. The system consumes 34.6 µW/74.3 µW for the glasses/watch and thus can be powered by the energy harvested from ambient light. We also test system’s robustness under varying light intensities, light directions, and ambient light fluctuations, where the system maintains high recognition accuracy (> 96%) in all tested settings. ",
        "authors": [
            {
                "name": "Yichen  Li, Dartmouth College"
            },
            {
                "name": "Tianxing  Li, Darmouth College"
            },
            {
                "name": "Ruchir A. Patel, Dartmouth College"
            },
            {
                "name": "Xing-Dong  Yang, Dartmouth College"
            },
            {
                "name": "Xia  Zhou, Dartmouth College"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "gesture recognition",
            "visible light sensing",
            "energy harvesting"
        ],
        "subtype": "paper",
        "title": "Self-Powered Gesture Recognition with Ambient Light",
        "type": "paper"
    },
    "ufp1319": {
        "abstract": "We present Indutivo, a contact-based inductive sensing technique for contextual interactions. Our technique recognizes conductive objects (metallic primarily) that are commonly found in households and daily environments, as well as their individual movements when placed against the sensor. These movements include sliding, hinging, and rotation. We describe our sensing principle and how we designed the size, shape, and layout of our sensor coils to optimize sensitivity, sensing range, recognition and tracking accuracy. Through several studies, we also demonstrated the performance of our proposed sensing technique in environments with varying levels of noise and interference conditions. We conclude by presenting demo applications on a smartwatch, as well as insights and lessons we learned from our experience. ",
        "authors": [
            {
                "name": "Jun  Gong, Dartmouth College"
            },
            {
                "name": "Xin  Yang, Dartmouth College & Renmin University of China"
            },
            {
                "name": "Teddy  Seyed, University of Calgary"
            },
            {
                "name": "Josh Urban Davis, Dartmouth College"
            },
            {
                "name": "Xing-Dong  Yang, Dartmouth College"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1319.jpg",
        "keywords": [
            "Wearable",
            "smartwatch",
            "inductive sensor"
        ],
        "subtype": "paper",
        "title": "Indutivo: Contact-Based, Object-Driven Interactions with Inductive Sensing",
        "type": "paper"
    },
    "ufp1329": {
        "abstract": "Today, radiologists diagnose three dimensional medical data using two dimensional displays. When designing environments with optimal conditions for such a process various aspects like contrast, screen reflection and background light have to be considered. As shown in previous research, applying virtual environments in combination with a Head-Mounted Display for diagnostic imaging provides potential benefits to reduce issues of bad posture and diagnostic mistakes. However, there is little research in exploring the usability and user experience of such beneficial environments.\n\nIn this work we designed and evaluated different means of interaction to increase radiologists' performance. Therefore we created a virtual reality radiology reading room and employed it to evaluate three different interaction techniques. These allow a direct, semi-direct and indirect manipulation for performing scrolling- and windowing- tasks which are the most important for a radiologist.\n\nA study including nine radiologists was conducted and evaluated using the User Experience Questionnaire. Results indicate that direct manipulation is the preferred interaction technique, it outscored the other two control possibilities in attractiveness and pragmatic quality.",
        "authors": [
            {
                "name": "Markus  Wirth, University of Erlangen-Nürnberg (FAU)"
            },
            {
                "name": "Stefan  Gradl, University of Erlangen-Nürnberg (FAU)"
            },
            {
                "name": "Jan  Sembdner, University of Erlangen-Nürnberg (FAU)"
            },
            {
                "name": "Soeren  Kuhrt, Siemens Healthcare GmbH"
            },
            {
                "name": "Bjoern M. Eskofier, University of Erlangen-Nürnberg (FAU)"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual Reality",
            "Interaction techniques",
            "Direct Interaction",
            "Indirect Interaction",
            "Diagnostic Radiology",
            "Virtual Environment"
        ],
        "subtype": "paper",
        "title": "Evaluation of Interaction Techniques for a Virtual Reality Reading Room in Diagnostic Radiology",
        "type": "paper"
    },
    "ufp1331": {
        "abstract": "Machine readable passive tags for tagging physical objects are ubiquitous today. We propose Motion Codes, a passive tagging mechanism that is based on the kinesthetic motion of the user's hand. Here, the tag comprises of a visual pattern that is displayed on a physical surface. To scan the tag and receive the encoded information, the user simply traces their finger over the pattern. The user wears an inertial motion sensing (IMU) ring on the finger that records the traced pattern. We design two motion code schemes, Asterisk and Obelisk that rely on directional vector data processed from the IMU. We evaluate both schemes for the effects of orientation, size, and data density on their accuracies. We further conduct an in-depth analysis of the sources of motion deviations in the ring data as compared to the ground truth finger movement data. Overall, Asterisk achieves a 95% accuracy for an information capacity of 16.8 million possible sequences.",
        "authors": [
            {
                "name": "Aakar  Gupta, University of Waterloo"
            },
            {
                "name": "Jiushan  Yang, University of Toronto"
            },
            {
                "name": "Ravin  Balakrishnan, University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1331.jpg",
        "keywords": [
            "Motion Code",
            "Machine Readable Tags",
            "Ring",
            "QR Code"
        ],
        "subtype": "paper",
        "title": "Asterisk and Obelisk: Motion Codes for Passive Tagging",
        "type": "paper"
    },
    "ufp1355": {
        "abstract": "Mavo is a small extension to the HTML language that empowers non-programmers to create simple web applications. Authors can mark up any normal HTML document with attributes that specify data elements that Mavo makes editable and persists. But while applications authored with Mavo allow users to edit individual data items, they do not offer any programmatic data actions that can act in customizable ways on large collections of data simultaneously or that modify data according to a computation. We explore an extension to the Mavo language that enables non-programmers to author these richer data update actions. We show that it lets authors create a more powerful set of applications than they could previously, while adding little additional complexity to the authoring process. Through user evaluations, we assess how closely our data update syntax matches how novice authors would instinctively express such actions, and how well they are able to use the syntax we provided.",
        "authors": [
            {
                "name": "Lea  Verou, Massachusetts Institute of Technology"
            },
            {
                "name": "Tarfah  Alrashed, Massachusetts Institute of Technology"
            },
            {
                "name": "David  Karger, Massachusetts Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1355.jpg",
        "keywords": [
            "Web design",
            "End-user programming",
            "Information architecture",
            "Semantic publishing",
            "Dynamic Media",
            "Web",
            "Query languages",
            "Data updates",
            "Reactive Programming"
        ],
        "subtype": "paper",
        "title": "Extending a Reactive Expression Language with Data Update Actions for End-User Application Authoring",
        "type": "paper"
    },
    "ufp1365": {
        "abstract": "We introduce I/O Braid, an interactive textile cord with embedded sensing and visual feedback. I/O Braid senses proximity, touch, and twist through a spiraling, repeating braiding topology of touch matrices. This sensing topology is uniquely scalable, requiring only a few sensing lines to cover the whole length of a cord. The same topology allows us to embed fiber optic strands to integrate co-located visual feedback.\n\nWe provide an overview of the enabling braiding techniques, design considerations, and approaches to gesture detection. These allow us to derive a set of interaction techniques, which we demonstrate with different form factors and capabilities. Our applications illustrate how I/O Braid can invisibly augment everyday objects, such as  touch-sensitive headphones and interactive drawstrings on garments, while enabling discoverability and feedback through embedded light sources. ",
        "authors": [
            {
                "name": "Alex  Olwal, Google Inc."
            },
            {
                "name": "Jon  Moeller, Google Inc."
            },
            {
                "name": "Greg  Priest-Dorman, Google Inc."
            },
            {
                "name": "Thad  Starner, Google Inc."
            },
            {
                "name": "Ben  Carroll, Google Inc."
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Interactive textiles",
            "sensor",
            "input",
            "wearables",
            "capacitive sensing",
            "soft electronics",
            "ubiquitous computing"
        ],
        "subtype": "paper",
        "title": "I/O Braid: Scalable Touch-Sensitive Lighted Cords Using Spiraling, Repeating Sensing Textiles and Fiber Optics",
        "type": "paper"
    },
    "ufp1371": {
        "abstract": "We present the first wireless physical analytics  system for 3D printed objects using commonly available conductive plastic filaments. Our design can enable various data capture and wireless physical analytics capabilities for 3D printed objects, without the need for  electronics. To achieve this goal, we make three key contributions: (1) demonstrate room scale backscatter communication and sensing using conductive plastic filaments, (2)  introduce the first backscatter designs that detect a variety of bi-directional motions and support linear and rotational movements, and (3)  enable data capture and storage for later retrieval when outside the range of the wireless coverage, using a ratchet and gear system. We validate our approach by wirelessly detecting the opening and closing of a pill bottle, capturing the joint angles of a 3D printed e-NABLE prosthetic hand, and an insulin pen that can store information to track its use outside the range of a wireless receiver. ",
        "authors": [
            {
                "name": "Vikram  Iyer, University of Washington"
            },
            {
                "name": "Justin  Chan, University of Washington"
            },
            {
                "name": "Ian  Culhane, University of Washington"
            },
            {
                "name": "Jennifer  Mankoff, University of Washington"
            },
            {
                "name": "Shyamnath  Gollakota, University of Washington"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1371.jpg",
        "keywords": [
            "3D printing",
            "digital fabrication",
            "backscatter",
            "prosthetic hand"
        ],
        "subtype": "paper",
        "title": "Wireless Analytics for 3D Printed Objects",
        "type": "paper"
    },
    "ufp1372": {
        "abstract": "The rise in prevalence of Internet of Things (IoT) technologies has encouraged more people to prototype and build custom internet connected devices based on low power microcontrollers. While well-developed tools exist for debugging network communication for desktop and web applications, it can be difficult for developers of networked embedded systems to figure out why their network code is failing due to the limited output affordances of embedded devices. This paper presents WiFröst , a new approach for debugging these systems using instrumentation that spans from the device itself, to its communication API, to the wireless router and back-end server. WiFröst automatically collects this data, displays it in a web-based visualization, and highlights likely issues with an extensible suite of checks based on analysis of recorded execution traces.",
        "authors": [
            {
                "name": "William  Mcgrath, Stanford University & University of California, Berkeley"
            },
            {
                "name": "Jeremy  Warner, University of California, Berkeley"
            },
            {
                "name": "Mitchell  Karchemsky, University of California, Berkeley"
            },
            {
                "name": "Andrew  Head, University of California, Berkeley"
            },
            {
                "name": "Daniel  Drew, University of California, Berkeley"
            },
            {
                "name": "Bjoern  Hartmann, University of California, Berkeley"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Internet of Things",
            "Debugging",
            "Embedded Systems",
            "IDE"
        ],
        "subtype": "paper",
        "title": "WiFröst: Bridging the Information Gap for Debugging of Networked Embedded Systems",
        "type": "paper"
    },
    "ufp1384": {
        "abstract": "We present CamTrackPoint, a new input interface that can be controlled by finger gestures captured by front or rear cameras of a mobile device.\nCamTrackPoint mounts a 3D-printed ring on the camera's bezel, and it senses the movements of the user's finger by tracking the light passed through the finger.\nThe proposed method provides mobile devices with a new input interface that offers physical force feedback like a pointing stick.\nThe cost of our method is low as it needs only a simple ring-shaped part on the camera bezel.\nMoreover, the ring doesn't disturb the functions of the camera, unless a user uses the interface. \nWe implement a prototype for a smartphone; two CamTrackPoint rings are made for the front and rear cameras.\nWe evaluate its performance and characteristics in an experiment.\nThe proposed technique provides smooth scrolling and would give better game experience on the available smartphone.",
        "authors": [
            {
                "name": "Wataru  Yamada, NTT DOCOMO"
            },
            {
                "name": "Hiroyuki  Manabe, NTT DOCOMO"
            },
            {
                "name": "Daizo  Ikeda, NTT DOCOMO"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1384.jpg",
        "keywords": [
            "Pointing stick",
            "gesture interface",
            "back-of-device interaction",
            "mobile"
        ],
        "subtype": "paper",
        "title": "CamTrackPoint: Camera-Based Pointing Stick Using Transmitted Light through Finger",
        "type": "paper"
    },
    "ufp1385": {
        "abstract": "Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable hand-to-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",
        "authors": [
            {
                "name": "Doyoung  Lee, UNIST"
            },
            {
                "name": "Youryang  Lee, UNIST"
            },
            {
                "name": "Yonghwan  Shin, UNIST"
            },
            {
                "name": "Ian  Oakley, UNIST"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1385.jpg",
        "keywords": [
            "Hand-to-Face Input",
            "Social Acceptability",
            "User Elicitation",
            "Augmented Reality",
            "Head Mounted Display"
        ],
        "subtype": "paper",
        "title": "Designing Socially Acceptable Hand-to-Face Input",
        "type": "paper"
    },
    "ufp1388": {
        "abstract": "Haptic feedback in VR is important for realistic simulation in virtual reality.  However, recreating the haptic experience for hand tools in VR traditionally requires hardware with precise actuators, adding complexity to the system.  We propose Ungrounded Haptic Retargeting, an interaction technique that provides a realistic haptic experience for grabbing tools using only passive mechanisms.  This technique leverages the ungrounded feedback inherent in grabbing tools combined with dynamic visual adjustments of their position in virtual reality to create an illusion of physical presence for virtual objects.  To demonstrate the capabilities of this technique, we created VR Grabbers, an exemplary passive VR controller, similar to training chopsticks, with haptic feedback for precise object selection and manipulation. \n\nWe conducted two user studies based on VR Grabbers.  The first study probed the perceptual limits of the illusion; we found that the maximum position difference between the virtual and physical world acceptable to the user is (-1.48, 1.95) cm.  The second study showed that task performance of the VR Grabbers controller with Ungrounded Haptic Retargeting enabled outperforms the same controller with Ungrounded Haptic Retargeting disabled. \n",
        "authors": [
            {
                "name": "Jackie (Junrui) Yang, Stanford University"
            },
            {
                "name": "Hiroshi  Horii, HP Inc."
            },
            {
                "name": "Alexander  Thayer, HP Inc."
            },
            {
                "name": "Rafael  Ballagas, HP Inc."
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1388.jpg",
        "keywords": [
            "Virtual reality",
            "Passive haptics",
            "Haptic Illusions",
            "Grabbing tools"
        ],
        "subtype": "paper",
        "title": "VR Grabbers: Ungrounded Haptic Retargeting for Precision Grabbing Tools",
        "type": "paper"
    },
    "ufp1390": {
        "abstract": "We make touch input by physically colliding an end effector (e.g., a body part or a stylus) with a touch surface. Prior studies have examined the use of kinematic variables of collision between objects, such as position, velocity, force, and impact. However, the nature of the collision can be understood more thoroughly by considering the known physical relationships that exist between directly measurable variables (i.e., kinetics). Based on this collision kinetics, this study proposes a novel touch technique called FDSense. By simultaneously observing the force and contact area measured from the touchpad, FDSense allows estimation of the Young’s modulus and stiffness of the object being contacted. Our technical evaluation showed that FDSense could effectively estimate the Young’s modulus of end effectors made of various materials, and the stiffness of each part of the human hand. Two applications using FDSense were demonstrated, for digital painting and digital instruments, where the result of the expression varies significantly depending on the elasticity of the end effector. In a following informal study, participants assessed the technique positively.",
        "authors": [
            {
                "name": "Sanghwa  Hong, Korea Institute of Science and Technology"
            },
            {
                "name": "Eunseok  Jeong, Korea Institute of Science and Technology"
            },
            {
                "name": "Seongkook  Heo, University of Toronto"
            },
            {
                "name": "Byungjoo  Lee, bjlee1985@gmail.com"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Young’s modulus",
            "stiffness",
            "touch interaction",
            "touch pad",
            "end effector",
            "contact mechanics",
            "kinetic interaction"
        ],
        "subtype": "paper",
        "title": "FDSense: Estimating Young's Modulus and Stiffness of End Effectors to Facilitate Kinetic Interaction on Touch Surfaces",
        "type": "paper"
    },
    "ufp1394": {
        "abstract": "This paper introduces Tacttoo, a feel-through interface for electro-tactile output on the user's skin.  Integrated in a temporary tattoo with a thin and conformal form factor, it can be applied on complex body geometries, including the fingertip, and is scalable to various body locations. At less than 35µm in thickness, it is the thinnest tactile interface for wearable computing to date. Our results show that Tacttoo retains the natural tactile acuity similar to bare skin while delivering high-density tactile output.\nWe present the fabrication of customized Tacttoo tattoos using DIY tools and contribute a mechanism for consistent electro-tactile operation on the skin.\nMoreover, we explore new interactive scenarios that are enabled by Tacttoo. Applications in tactile augmented reality and on-skin interaction benefit from a seamless augmentation of real-world tactile cues with computer-generated stimuli. Applications in virtual reality and private notifications benefit from high-density output in an ergonomic form factor.\nResults from two psychophysical studies and a technical evaluation demonstrate Tacttoo's functionality, feel-through properties and durability.",
        "authors": [
            {
                "name": "Anusha  Withana,  Saarland University, Saarland Informatics Campus"
            },
            {
                "name": "Daniel  Groeger, Saarland University, Saarland Informatics Campus"
            },
            {
                "name": "Jürgen  Steimle, Saarland University, Saarland Informatics Campus"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1394.jpg",
        "keywords": [
            "On-Body Interaction",
            "Skin",
            "Tactile Display",
            "Electro-Tactile",
            "Tattoo",
            "Fabrication",
            "Printed Electronics",
            "Wearable Computing"
        ],
        "subtype": "paper",
        "title": "Tacttoo: A Thin and Feel-Through Tattoo for On-Skin Tactile Output",
        "type": "paper"
    },
    "ufp1395": {
        "abstract": "End-to-end latency in interactive systems is detrimental to performance and usability, and comes from a combination of hardware and software delays. While these delays are steadily addressed by hardware and software improvements, it is at a decelerating pace. In parallel, short-term input prediction has shown promising results in recent years, in both research and industry, as an addition to these efforts. We describe a new prediction algorithm for direct touch devices based on (i) a state-of-the-art finite-time derivative estimator, (ii) a smoothing mechanism based on input speed, and (iii) a post-filtering of the prediction in two steps. Using both a pre-existing dataset of touch input as benchmark, and subjective data from a new user study, we show that this new predictor outperforms the predictors currently available in the literature and industry, based on metrics that model user-defined negative side-effects caused by input prediction. In particular, we show that our predictor can predict up to 2 or 3 times further than existing techniques with minimal negative side-effects.",
        "authors": [
            {
                "name": "Mathieu  Nancel, Inria & Université de Lille"
            },
            {
                "name": "Stanislav  Aranovskiy, CentraleSupelec"
            },
            {
                "name": "Rosane  Ushirobira, Inria & Université de Lille"
            },
            {
                "name": "Denis  Efimov, Inria & Université de Lille"
            },
            {
                "name": "Sebastien  Poulmane, Inria & Université de Lille"
            },
            {
                "name": "Nicolas  Roussel, Inria & Université de Lille"
            },
            {
                "name": "Géry  Casiez, Université de Lille & Inria"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1395.jpg",
        "keywords": [
            "touch input",
            "latency",
            "lag",
            "prediction technique"
        ],
        "subtype": "paper",
        "title": "Next-Point Prediction for Direct Touch Using Finite-Time Derivative Estimation",
        "type": "paper"
    },
    "ufp1402": {
        "abstract": "Grabbing users' attention is a fundamental aspect of interactive systems. However, there is a disconnect between the ways our devices notify us and how our bodies do so naturally. In this paper, we explore the body's modality of itching as a way to provide such natural feedback. We create itching sensations via low-current electric stimulation, which allows us to quickly generate this sensation on demand. In a first study we explore the design space around itching and how changes in stimulation parameters influence the resulting sensation. In a second study we compare vibration feedback and itching integrated in a smartwatch form factor. We find that we can consistently induce itching sensations and that these are perceived as more activating and interrupting than vibrotactile stimuli.",
        "authors": [
            {
                "name": "Henning  Pohl, University of Copenhagen"
            },
            {
                "name": "Kasper  Hornbæk, University of Copenhagen"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1402.jpg",
        "keywords": [
            "itch feedback",
            "haptics",
            "wearables",
            "on-body interfaces",
            "skin"
        ],
        "subtype": "paper",
        "title": "ElectricItch: Skin Irritation as a Feedback Modality",
        "type": "paper"
    },
    "ufp1410": {
        "abstract": "When developing a real-walking virtual reality experience, designers generally create virtual locations to fit a specific tracking volume. Unfortunately, this prevents the resulting experience from running on a smaller or differently shaped tracking volume. To address this, we present a software system called Scenograph. The core of Scenograph is a tracking volume-independent representation of real-walking experiences. Scenograph instantiates the experience to a tracking volume of given size and shape by splitting the locations into smaller ones while maintaining narrative structure. In our user study, participants’ ratings of realism decreased significantly when existing techniques were used to map a 25m2 experience to 9m2 and an L-shaped 8m2 tracking volume. In contrast, ratings did not differ when Scenograph was used to instantiate the experience.",
        "authors": [
            {
                "name": "Sebastian  Marwecki, University of Potsdam"
            },
            {
                "name": "Patrick  Baudisch, University of Potsdam"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1410.jpg",
        "keywords": [
            "Virtual reality",
            "real-walking",
            "locomotion"
        ],
        "subtype": "paper",
        "title": "Scenograph: Fitting Real-Walking VR Experiences into Various Tracking Volumes",
        "type": "paper"
    },
    "ufp1414": {
        "abstract": "We present RESi (Resistive tExtile Sensor Interfaces), a novel sensing approach enabling a new kind of yarn-based, resistive pressure sensing. The core of RESi builds on a newly designed yarn, which features conductive and resistive properties. We run a technical study to characterize the behaviour of the yarn and to determine the sensing principle. We demonstrate how the yarn can be used as a pressure sensor and discuss how specific issues, such as connecting the soft textile sensor with the rigid electronics can be solved. In addition, we present a platform-independent API that allows rapid prototyping. To show its versatility, we present applications developed with different textile manufacturing techniques, including hand sewing, machine sewing, and weaving. RESi is a novel technology, enabling textile pressure sensing to augment everyday objects with interactive capabilities.",
        "authors": [
            {
                "name": "Patrick  Parzer, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Florian  Perteneder, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Kathrin  Probst, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Christian  Rendl, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Joanne  Leong, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Sarah  Schuetz, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Anita  Vogl, University of Applied Sciences Upper Austria"
            },
            {
                "name": "Reinhard  Schwoediauer, Johannes Kepler University"
            },
            {
                "name": "Martin  Kaltenbrunner, Johannes Kepler University"
            },
            {
                "name": "Siegfried  Bauer, Johannes Kepler University"
            },
            {
                "name": "Michael  Haller, University of Applied Sciences Upper Austria"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1414.jpg",
        "keywords": [
            "Wearable Computing",
            "Interactive Textiles",
            "Textile Sensor",
            "Conductive Yarns",
            "Manufacturing"
        ],
        "subtype": "paper",
        "title": "RESi: A Highly Flexible, Pressure-Sensitive, Imperceptible Textile Interface Based on Resistive Yarns",
        "type": "paper"
    },
    "ufp1416": {
        "abstract": "Many web pages developed today require navigation by visual interaction—seeing,  hovering, pointing,  clicking, and dragging with the mouse over dynamic page content.  These forms of interaction are increasingly popular as developer trends have moved from static, logically structured pages to dynamic, interactive pages. However, they are also often inaccessible to blind web users who tend to rely on keyboard-based  screen  readers  to  navigate  the  web. Despite  existing web accessibility standards, engineering web pages to be equally accessible via both keyboard and visuomotor mouse-based interactions is often not a priority for developers. Improving access to this kind of visual and interactive web content has been a long-standing  goal of HCI researchers, but the barriers have proven to be too varied and unpredictable to be overcome by some of the proposed solutions: promoting  guidelines and  best  practices,  automatically  generating accessible versions of pre-exisiting web pages, or developing human-assisted solutions, such as screen and cursor-sharing, which  tend  to  diminish  an  end  user’s  agency. In this paper we present a real-time, collaborative approach to helping blind web users overcome inaccessible parts of existing web pages.  We introduce *Arboretum*, a new architecture that enables  any  web user to seamlessly hand off controlled parts of their browsing session to remote users, while maintaining control over the interface via a \"propose and accept/reject\"mechanism. We  illustrate  the  benefit of Arboretum by using it to implement *Arbility*, a browser that allows blind users to hand off targeted visual interaction tasks to remote crowd workers. We  evaluate the entire system in a study  with 9 blind web users, showing that Arbility allows them to interact with web content that was previously difficult to access via a screen reader alone.",
        "authors": [
            {
                "name": "Steve  Oney, University of Michigan"
            },
            {
                "name": "Alan  Lundgard, University of Michigan"
            },
            {
                "name": "Rebecca  Krosnick, University of Michigan"
            },
            {
                "name": "Michael  Nebeling, University of Michigan"
            },
            {
                "name": "Walter S. Lasecki, University of Michigan"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Accessibility",
            "Web Accessibility",
            "Non-visual Access",
            "Blind",
            "Web Interfaces",
            "Remote Collaboration",
            "Crowdsourcing"
        ],
        "subtype": "paper",
        "title": "Arboretum and Arbility: Improving Web Accessibility Through a Shared Browsing Architecture",
        "type": "paper"
    },
    "ufp1442": {
        "abstract": "Recently, researchers have developed black-box approaches to mine design and interaction data from mobile apps. Although the data captured during this interaction mining is descriptive, it does not expose the design semantics of UIs: what elements on the screen mean and how they are used. This paper introduces an automatic approach for generating semantic annotations for mobile app UIs. Through an iterative open coding of 73k UI elements and 720 screens, we contribute a lexical database of 25 types of UI components, 197 text button concepts, and 135 icon classes shared across apps. We use this labeled data to learn code-based patterns to detect UI components and to train a convolutional neural network that distinguishes between icon classes with 94% accuracy. To demonstrate the efficacy of our approach at scale, we compute semantic annotations for the 72k unique UIs in the Rico dataset, assigning labels for 78% of the total visible, non-redundant elements.",
        "authors": [
            {
                "name": "Thomas F. Liu, University of Illinois at Urbana–Champaign"
            },
            {
                "name": "Mark  Craft, University of Illinois at Urbana–Champaign"
            },
            {
                "name": "Jason  Situ, University of Illinois at Urbana–Champaign"
            },
            {
                "name": "Ersin  Yumer, Argo AI"
            },
            {
                "name": "Radomir  Mech, Adobe Research"
            },
            {
                "name": "Ranjitha  Kumar, University of Illinois at Urbana-Champaign"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1442.jpg",
        "keywords": [
            "Design semantics",
            "mobile app design",
            "machine learning"
        ],
        "subtype": "paper",
        "title": "Learning Design Semantics for Mobile Apps",
        "type": "paper"
    },
    "ufp1443": {
        "abstract": "In this paper, we present Touch+Finger, a new interaction technique that augments touch input with multi-finger gestures for rich and expressive interaction. The main idea is that while one finger is engaged in a touch event, a user can leverage the remaining fingers, the “idle” fingers, to perform a variety of hand poses or in-air gestures to extend touch-based user interface capabilities. To fully understand the use of these idle fingers, we constructed a design space based on conventional touch gestures (i.e., single- and multi-touch gestures) and inter- action period (i.e., before and during touch). Considering the design space, we investigated the possible movement of the idle fingers and developed a total of 20 Touch+Finger gestures. Using ring-like devices to track the motion of the idle fingers in the air, we evaluated the Touch+Finger gestures on both recognition accuracy and ease of use. They were classified with a recognition accuracy of over 99% and received positive and negative comments from 8 participants. We suggested 8 interaction techniques with Touch+Finger gestures that demonstrate extended touch-based user interface capabilities.",
        "authors": [
            {
                "name": "Hyunchul  Lim, Seoul National University"
            },
            {
                "name": "Jungmin  Chung, Seoul National University"
            },
            {
                "name": "Changhoon  Oh, Seoul National University"
            },
            {
                "name": "Sohyun  Park, Seoul National University"
            },
            {
                "name": "Joonhwan  Lee, Seoul National University"
            },
            {
                "name": "Bongwon  Suh, Seoul National University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Touch input",
            "gestural interaction",
            "interaction techniques, wearable rings",
            "joint interaction"
        ],
        "subtype": "paper",
        "title": "Touch+Finger: Extending Touch-based User Interface Capabilities with “Idle” Finger Gestures in the Air",
        "type": "paper"
    },
    "ufp1451": {
        "abstract": "We introduce a new pen input space by forming postures with the same hand that also grips the pen while writing, drawing, or selecting. The postures contact the multitouch surface around the pen to enable detection without special sensors. A formative study investigates the effectiveness, accuracy, and comfort of 33 candidate postures in controlled tasks. The results indicate a useful subset of postures. Using raw capacitive sensor data captured in the study, a convolutional neural network is trained to recognize 10 postures in real time. This recognizer is used to create application demonstrations for pen-based document annotation and vector drawing. A small usability study shows the approach is feasible.",
        "authors": [
            {
                "name": "Drini  Cami, University of Waterloo"
            },
            {
                "name": "Fabrice  Matulic, Preferred Networks Inc. & University of Waterloo"
            },
            {
                "name": "Richard G. Calland, Preferred Networks Inc."
            },
            {
                "name": "Brian  Vogel, Preferred Networks Inc."
            },
            {
                "name": "Daniel  Vogel, University of Waterloo"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1451.jpg",
        "keywords": [
            "pen input",
            "touch input",
            "interaction techniques"
        ],
        "subtype": "paper",
        "title": "Unimanual Pen+Touch Input Using Variations of Precision Grip Postures",
        "type": "paper"
    },
    "ufp1454": {
        "abstract": "Since the advent of consumer photography, tourists and hikers have made photo records of their trips to share later. Aside from being kept as memories, photo presentations such as slideshows are also shown to others who have not visited the location to try to convey the experience.However, a slideshow alone is limited in conveying the broader spatial context, and thus the feeling of presence in beautiful natural scenery is lost. We address this by presenting the photographs as part of an immersive experience. We introduce an automated pipeline for aligning photographs with a digital terrain model. From this geographic registration, we produce immersive presentations which are viewed either passively as a video, or interactively in virtual reality. Our experimental evaluation verifies that this new mode of presentation successfully conveys the spatial context of the scene and is enjoyable to users.",
        "authors": [
            {
                "name": "Jan  Brejcha, Brno University of Technology & Adobe Research"
            },
            {
                "name": "Michal  Lukác, Adobe Research"
            },
            {
                "name": "Zhili  Chen, Adobe Research"
            },
            {
                "name": "Stephen  Diverdi, Adobe Research"
            },
            {
                "name": "Martin  Cadík, Brno University of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Photography presentation",
            "hike",
            "image geo-localization",
            "terrain model",
            "immersive visualization"
        ],
        "subtype": "paper",
        "title": "Immersive Trip Reports",
        "type": "paper"
    },
    "ufp1456": {
        "abstract": "Screencasts, where recordings of a computer screen are broadcast to a large audience on the web, are becoming popular as an online educational tool. To provide rich interactions with the text within screencasts, there are emerging platforms that support text-based screencasts by recording every character insertion and deletion from the creator and reconstructing its playback on the viewer's screen. However, these platforms lack support for non-linear editing of screencasts, which involves manipulating a sequence of text editing operations. Since text editing operations are tightly coupled in sequence, modifying an arbitrary part of the sequence often creates ambiguity that yields multiple possible results that require user's choice for resolution.\nWe present an editing tool with a non-linear editing algorithm for text-based screencasts. The tool allows users to edit any arbitrary part of a text-based screencast while preserving the overall consistency of the screencast. In an exploratory user study, all subjects successfully carried out a variety of screencast editing tasks using our prototype screencast editor.",
        "authors": [
            {
                "name": "Jungkook  Park, Korea Institute of Science and Technology"
            },
            {
                "name": "Yeong Hoon  Park, University of Minnesota"
            },
            {
                "name": "Alice  Oh, Korea Advanced Institute of Science and Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Screencast, Screencast editing",
            "Non-linear editing",
            "Operational transformation,Screencast",
            "Screencast editing",
            "Non-linear editing",
            "Operational transformation"
        ],
        "subtype": "paper",
        "title": "Non-Linear Editing of Text-Based Screencasts",
        "type": "paper"
    },
    "ufp1460": {
        "abstract": "We introduce MetaArms, wearable anthropomorphic robotic arms and hands with six degrees of freedom operated by the user’s legs and feet. Our overall research goal is to re-imagine what our bodies can do with the aid of wearable robotics using a body-remapping approach. To this end, we present an initial exploratory case study. MetaArms’ two robotic arms are controlled by the user’s feet motion, and the robotic hands can grip objects according to the user’s toes bending. Haptic feedback is also presented on the user’s feet that correlate with the touched objects on the robotic hands, creating a closed-loop system. We present formal and informal evaluations of the system, the former using a 2D pointing task according to Fitts’ Law. The overall throughput for 12 users of the system is reported as 1.01 bits/s (std 0.39). We also present informal feedback from over 230 users. We find that MetaArms demonstrate the feasibility of body-remapping approach in designing robotic limbs that may help us re-imagine what the human body could do.",
        "authors": [
            {
                "name": "Mhd Yamen  Saraiji, Keio University"
            },
            {
                "name": "Tomoya  Sasaki, The University of Tokyo"
            },
            {
                "name": "Kai  Kunze, Keio University"
            },
            {
                "name": "Kouta  Minamizawa, Keio University"
            },
            {
                "name": "Masahiko  Inami, The University of Tokyo"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1460.jpg",
        "keywords": [
            "Artificial Limbs",
            "Feet Interactions",
            "Body Remapping",
            "Body Schema",
            "Fitts' Law",
            "Augmented Arms",
            "Human Enhancement"
        ],
        "subtype": "paper",
        "title": "MetaArms: Body Remapping Using Feet-Controlled Artificial Arms",
        "type": "paper"
    },
    "ufp1466": {
        "abstract": "From smart toys and household appliances to personal robots, electromechanical devices play an increasingly important role in our daily lives. Rather than relying on gadgets that are mass-produced, our goal is to enable casual users to custom-design such devices based on their own needs and preferences. To this end, we present a computational design system that leverages the power of digital fabrication and the emergence of affordable electronics such as sensors and microcontrollers. The input to our system consists of a 3D representation of the desired device's shape, and a set of user-preferred off-the-shelf components. Based on this input, our method generates an optimized, 3D printable enclosure that can house the required components. To create these designs automatically, we formalize a new spatio-temporal model that captures the entire assembly process, including the placement of the components within the device, mounting structures and attachment strategies, the order in which components must be inserted, and collision-free assembly paths. Using this model as a technical core, we then leverage engineering design guidelines and efficient numerical techniques to optimize device designs. In a user study, which also highlights the challenges of designing such devices, we find our system to be effective in reducing the entry barriers faced by casual users in creating such devices. We further demonstrate the versatility of our approach by designing and fabricating three devices with diverse functionalities.",
        "authors": [
            {
                "name": "Ruta  Desai, Carnegie Mellon University"
            },
            {
                "name": "James  Mccann, Carnegie Mellon University"
            },
            {
                "name": "Stelian  Coros, ETH Zurich"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1466.jpg",
        "keywords": [
            "Computational Design",
            "Digital Fabrication",
            "Optimization"
        ],
        "subtype": "paper",
        "title": "Assembly-aware Design of Printable Electromechanical Devices",
        "type": "paper"
    },
    "ufp1472": {
        "abstract": "When one manipulates a large or bulky object, s/he utilizes tactile information at both fingers and the palm. Our goal is to efficiently convey contact information to a user’s hand during interaction with a virtual object. We propose a haptic system that can provide haptic feedback to thumb/middle finger/index finger and on a palm. Our interface design utilizes a novel compact mechanism to provide haptic information to the palm. Also, we propose a haptic rendering strategy to calculate haptic feedback continuously. We demonstrate that cutaneous feedback on the palm improves the haptic perception of a large virtual object compared to when there is only kinesthetic feedback to the fingers.",
        "authors": [
            {
                "name": "Bukun  Son, Korea Institute of Science and Technology"
            },
            {
                "name": "Jaeyoung  Park, Korea Institute of Science and Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Haptics",
            "Wearable Interface",
            "Global Shape Perception"
        ],
        "subtype": "paper",
        "title": "Haptic Feedback to the Palm and Fingers for Improved Tactile Perception of Large Objects",
        "type": "paper"
    },
    "ufp1479": {
        "abstract": "We introduce DextrES, a flexible and wearable haptic glove which integrates both kinesthetic and cutaneous feedback in a thin and light form factor (weight is less than 8g). \nOur approach is based on an electrostatic clutch generating up to 20 N of holding force on each finger by modulating the electrostatic attraction between flexible elastic metal strips to generate an electrically-controlled friction force. We harness the resulting braking force to rapidly render on-demand kinesthetic feedback. The electrostatic brake is mounted onto the the index finger and thumb via modular 3D printed articulated guides which allow the metal strips to glide smoothly.  Cutaneous feedback is provided via piezo actuators at the fingertips. We demonstrate that our approach can provide rich haptic feedback under dexterous articulation of the user's hands and provides effective haptic feedback across a variety of different grasps. A controlled experiment indicates that DextrES improves the grasping precision for different types of virtual objects. Finally, we report on results of a psycho-physical study which identifies discrimination thresholds for different levels of holding force.  ",
        "authors": [
            {
                "name": "Ronan  Hinchet, EPFL"
            },
            {
                "name": "Velko  Vechev, ETH"
            },
            {
                "name": "Herbert  Shea, EPFL"
            },
            {
                "name": "Otmar  Hilliges, ETH"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1479.jpg",
        "keywords": [
            "Haptics",
            "VR",
            "electrostatic brake",
            "dexterous interaction"
        ],
        "subtype": "paper",
        "title": "DextrES: Wearable Haptic Feedback for Grasping in VR via a Thin Form-Factor Electrostatic Brake",
        "type": "paper"
    },
    "ufp1482": {
        "abstract": "We present GridDrones, a self-levitating programmable matter platform that can be used for representing 2.5D voxel grid relief maps capable of rendering unsupported structures and 3D transformations. GridDrones consists of cube-shaped nanocopters that can be placed in a volumetric 1xnxn mid-air grid, which is demonstrated here with 15 voxels. The number of voxels and scale is only limited by the size of the room and budget. Grid deformations can be applied interactively to this voxel lattice by manually selecting a set of voxels, then assigning a continuous topological relationship between voxel sets that determines how voxels move in relation to each other and manually drawing out selected voxels from the lattice structure. Using this simple technique, it is possible to create unsupported structures that can be translated and oriented freely in 3D. Shape transformations can also be recorded to allow for simple physical shape morphing animations. This work extends previous work on selection and editing techniques for 3D user interfaces. ",
        "authors": [
            {
                "name": "Sean  Braley, Queen's University"
            },
            {
                "name": "Calvin  Rubens, Queen's University"
            },
            {
                "name": "Timothy  Merritt, Aalborg University"
            },
            {
                "name": "Roel  Vertegaal, Queen's University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Organic User Interfaces",
            "Claytronics",
            "Radical Atoms",
            "Programmable Matter",
            "Swarm User Interfaces"
        ],
        "subtype": "paper",
        "title": "GridDrones: A Self-Levitating Physical Voxel Lattice for Interactive 3D Surface Deformations",
        "type": "paper"
    },
    "ufp1488": {
        "abstract": "Fact-checking, the task of assessing the veracity of claims, is an important, timely, and challenging problem. While many automated fact-checking systems have been recently proposed, the human side of the partnership has been largely neglected: how might people understand, interact with, and establish trust with an AI fact-checking system? Does such a system actually help people better assess the factuality of claims? In this paper, we present the design and evaluation of a mixed-initiative approach to fact-checking, blending human knowledge and experience with the efficiency and scalability of automated information retrieval and ML. In a user study in which participants used our system to aid their own assessment of claims, our results suggest that individuals tend to trust the system: participant accuracy assessing claims improved when exposed to correct model predictions. However, this trust perhaps goes too far: when the model was wrong, exposure to its predictions often degraded human accuracy. Participants given the option to interact with these incorrect predictions were often able improve their own performance. This suggests that transparent models are key to facilitating effective human interaction with fallible AI models.",
        "authors": [
            {
                "name": "An T. Nguyen, University of Texas at Austin"
            },
            {
                "name": "Aditya  Kharosekar, University of Texas at Austin"
            },
            {
                "name": "Saumyaa  Krishnan, University of Texas at Austin"
            },
            {
                "name": "Siddhesh  Krishnan, University of Texas at Austin"
            },
            {
                "name": "Elizabeth  Tate, University of Texas at Austin"
            },
            {
                "name": "Byron C. Wallace, Northeastern University"
            },
            {
                "name": "Matthew  Lease, University of Texas at Austin"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "AI",
            "Mixed-initiative",
            "Fact-checking",
            "Information Literacy"
        ],
        "subtype": "paper",
        "title": "Believe it or not: Designing a Human-AI Partnership for Mixed-Initiative Fact-Checking",
        "type": "paper"
    },
    "ufp1499": {
        "abstract": "This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.",
        "authors": [
            {
                "name": "Ryo  Suzuki, University of Colorado Boulder"
            },
            {
                "name": "Junichi  Yamaoka, The University of Tokyo"
            },
            {
                "name": "Daniel  Leithinger, University of Colorado Boulder"
            },
            {
                "name": "Tom  Yeh, University of Colorado Boulder"
            },
            {
                "name": "Mark D. Gross, University of Colorado Boulder"
            },
            {
                "name": "Yoshihiro  Kawahara, The University of Tokyo"
            },
            {
                "name": "Yasuaki  Kakehi, The University of Tokyo"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1499.jpg",
        "keywords": [
            "dynamic 3d printing",
            "shape displays",
            "digital materials"
        ],
        "subtype": "paper",
        "title": "Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",
        "type": "paper"
    },
    "ufp1501": {
        "abstract": "Current haptic devices are often bulky and rigid, making them unsuitable for ubiquitous interaction and scenarios where the user must also interact with the real world. To address this gap, we propose HydroRing, an unobtrusive, finger-worn device that can provide the tactile sensations of pressure, vibration, and temperature on the fingertip, enabling mixed-reality haptic interactions. Different from previous explorations, HydroRing in active mode delivers sensations using liquid travelling through a thin, flexible latex tube worn across the fingerpad, and has minimal impact on a user’s dexterity and their perception of stimuli in passive mode. Two studies evaluated participants’ ability to perceive and recognize sensations generated by the device, as well as their ability to perceive physical stimuli while wearing the device. We conclude by exploring several applications leveraging this mixed-reality haptics approach.",
        "authors": [
            {
                "name": "Teng  Han, Autodesk Research & University of Manitoba"
            },
            {
                "name": "Fraser  Anderson, Autodesk Research"
            },
            {
                "name": "Pourang  Irani, University of Manitoba"
            },
            {
                "name": "Tovi  Grossman, Autodesk Research & University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Haptics",
            "mixed reality",
            "ubiquitous interaction"
        ],
        "subtype": "paper",
        "title": "HydroRing: Supporting Mixed Reality Haptics Using Liquid Flow",
        "type": "paper"
    },
    "ufp1537": {
        "abstract": "While many online resources teach basic web development, few are designed to help novices learn the CSS concepts and design patterns experts use to implement complex visual features. Professional webpages embed these design patterns and could serve as rich learning materials, but their stylesheets are complex and difficult for novices to understand. This paper presents Ply, a CSS inspection tool that helps novices use their visual intuition to make sense of professional webpages. We introduce a new emph{visual relevance testing} technique to identify properties that have visual effects on the page, which Ply uses to hide visually irrelevant code and surface unintuitive relationships between properties. In user studies, Ply helped novice developers replicate complex web features 50% faster than those using Chrome Developer Tools, and allowed novices to recognize and explain unfamiliar concepts. These results show that visual inspection tools can support learning from complex professional webpages, even for novice developers.",
        "authors": [
            {
                "name": "Sarah  Lim, Northwestern University"
            },
            {
                "name": "Joshua  Hibschman, Northwestern University"
            },
            {
                "name": "Haoqi  Zhang, Northwestern University"
            },
            {
                "name": "Eleanor  O'Rourke, Northwestern University"
            }
        ],
        "award": false,
        "hm": false,
        "image_url": "http://uist.acm.org/uist2018/images/thumbnails/ufp1537.jpg",
        "keywords": [
            "Developer tools",
            "web inspection",
            "CSS",
            "authentic learning"
        ],
        "subtype": "paper",
        "title": "Ply: A Visual Web Inspector for Learning from Professional Webpages",
        "type": "paper"
    },
    "ujrnl01": {
        "abstract": "When fabricating, it is common to follow a prescribed set of steps in a tutorial or how-to. While popular, such explicit knowledge resources have many inconsistencies and omissions, use static illustrations, and cannot adapt to drop-in makers or a maker's mistakes. To overcome many of these issues, this work presents Automatics, a novel explicit knowledge resource system that dynamically generates fabrication activities for one or more makers based on their current environmental and fabrication context. Automatics assigns tasks to makers based on the past tools and components the maker was working with, enables makers to recover from mistakes through model regeneration, suggests alternative tools if a needed tool is unavailable or in use, and allows multiple makers to drop-in throughout a fabrication activity. Initial usage and feedback from novice makers showed that Automatics increases the number of tasks that can be completed compared to paper instructions, decreases frustration, and improves one's understanding of the global context of assigned tasks during fabrication activities.",
        "authors": [
            {
                "name": "Matthew  Lakier, University of Toronto"
            },
            {
                "name": "Michelle  Annett, MishMashMakers"
            },
            {
                "name": "Daniel  Wigdor, University of Toronto"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "TOCHI article",
        "title": "TOCHI: Automatics: Dynamically Generating Fabrication Tasks to Adapt to Varying Contexts",
        "type": "TOCHI article"
    },
    "ujrnl02": {
        "abstract": "Discovering gestures that gain consensus is a key goal of gesture elicitation. To this end, HCI research has developed statistical methods to reason about agreement. We review these methods and identify three major problems. First, we show that raw agreement rates disregard agreement that occurs by chance and do not reliably capture how participants distinguish among referents. Second, we explain why current recommendations on how to interpret agreement scores rely on problematic assumptions. Third, we demonstrate that significance tests for comparing agreement rates, either within or between participants, yield large Type I error rates (>40% for α =.05). As alternatives, we present agreement indices that are routinely used in inter-rater reliability studies. We discuss how to apply them to gesture elicitation studies. We also demonstrate how to use common resampling techniques to support statistical inference with interval estimates. We apply these methods to reanalyze and reinterpret the findings of four gesture elicitation studies.",
        "authors": [
            {
                "name": "Theophanis  Tsandilas, Inria, Université Paris-Saclay, and Univ. Paris-Sud"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "TOCHI article",
        "title": "TOCHI: Fallacies of Agreement: A Critical Review of Consensus Assessment Methods for Gesture",
        "type": "TOCHI article"
    },
    "upp1021": {
        "abstract": "Momentary switches are important building blocks to prototype novel physical user interfaces and enable tactile, explicit and eyes-free interactions. Unfortunately, typical representatives, such as push-buttons or pre-manufactured membrane switches, often do not fulfill individual design requirements and lack customization options for rapid prototyping. With this work, we present Pushables, a DIY fabrication approach for producing thin, bendable and highly customizable membrane dome switches. Therefore, we contribute a three-stage fabrication pipeline that describes the production and assembly on the basis of prototyping methods with different skill levels making our approach suitable for technology-enthusiastic makers, researchers, fab labs and others who require custom membrane switches in small quantities. To demonstrate the wide applicability of Pushables, we present application examples from ubiquitous, mobile and wearable computing.",
        "authors": [
            {
                "name": "Konstantin  Klamka, Technische Universität Dresden"
            },
            {
                "name": "Raimund  Dachselt, Technische Universität Dresden"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "paper button",
            "tactile",
            "printed electronics",
            "interactive paper",
            "haptic",
            "membrane switch",
            "dome switch",
            ""
        ],
        "subtype": "poster",
        "title": "Pushables: A DIY Approach for Fabricating Customizable and Self-Contained Tactile Membrane Dome Switches",
        "type": "poster"
    },
    "upp1022": {
        "abstract": "This paper presents Mindgame, a reinforcement learning optimized neurofeedback mindfulness system. To avoid the potential bias and difficulties of designing mapping between neural signal and output, we adopt a trial-and-error learning method to explore the preferred mapping. In a pilot study we assess the effectiveness of Mindgame in mediating people’s EEG alpha band. All participants’ alpha band change towards the desired direction.",
        "authors": [
            {
                "name": "Tongda  Xu, New York University"
            },
            {
                "name": "Dinglu  Wang, Tsinghua University"
            },
            {
                "name": "Xiaohui  You, Tsinghua University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Neurofeedback",
            "Reinforcement Learning",
            "Brain-computer Interface"
        ],
        "subtype": "poster",
        "title": "Mindgame: Mediating People’s EEG Alpha Band Power through Reinforcement Learning",
        "type": "poster"
    },
    "upp1034": {
        "abstract": "Today\"s smartphone notification systems are incapable of determining whether a notification has been successfully perceived without explicit interaction from the user. When the system incorrectly assumes that a notification has not been perceived, it may repeat it redundantly, disrupting the user (e.g., phone ringing). Or, when it assumes that a notification was perceived, and therefore fails to repeat it, the notification will be missed altogether (e.g., text message). We introduce SweatSponse, a feedback loop using skin conductance responses (SCR) to infer the perception of smartphone notifications just after their presentation. Early results from a laboratory study suggest that notifications induce SCR and that they could be used to better infer perception of smartphone notifications in real-time.",
        "authors": [
            {
                "name": "Pascal E. Fortin, McGill University"
            },
            {
                "name": "Elisabeth  Sulmont, McGill University"
            },
            {
                "name": "Jeremy R. Cooperstock, McGill University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Electrodermal activity",
            "notifications",
            "biosignal interactions"
        ],
        "subtype": "poster",
        "title": "SweatSponse: Closing the Loop on Notification Delivery Using Skin Conductance Responses",
        "type": "poster"
    },
    "upp1039": {
        "abstract": "We present SurfaceStreams, an open-source toolkit for recording and sharing visual content among multiple heterogeneous display-camera systems. SurfaceStreams clients support on-the-fly background removal and rectification on a range of different capture devices (Kinect & RealSense depth cameras, SUR40 sensor, plain webcam). After preprocessing, the raw data is compressed and sent to the SurfaceStreams server, which can dynamically receive streams from multiple clients, overlay them using the removed background as mask, and deliver the merged result back to the clients for display. We discuss an exemplary usage scenario (3-way shared interactive tabletop surface) and present results from a preliminary performance evaluation.",
        "authors": [
            {
                "name": "Florian  Echtler, Bauhaus-Universität Weimar"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "interactive surfaces",
            "tabletop",
            "video streaming",
            "telepresence"
        ],
        "subtype": "poster",
        "title": "SurfaceStreams: A Content-Agnostic Streaming Toolkit for Interactive Surfaces",
        "type": "poster"
    },
    "upp1040": {
        "abstract": "Augmented collaboration in a shared house design scenario has been studied widely with various approaches. However, those studies did not consider human perception. Our goal is to lower the user’s perceptual load for augmented collaboration in shared space design scenarios. Applying attention theories, we implemented shared head gaze, shared selected object, and collaborative manipulation features in our system in two different versions with HoloLens. To investigate whether user perceptions of the two different versions differ, we conducted an experiment with 18 participants (9 pairs) and conducted a survey and semi-structured interviews. The results did not show significant differences between the two versions, but produced interesting insights. Based on the findings, we provide design guidelines for collaborative AR systems. ",
        "authors": [
            {
                "name": "Yoonjeong  Cha, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Sungu  Nam, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Mun Yong  Yi, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Jaeseung  Jeong, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Woontack  Woo, Korea Advanced Institute of Science & Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Collaborative AR",
            "Human Perception",
            "Shared Space Design"
        ],
        "subtype": "poster",
        "title": "Augmented Collaboration in Shared Space Design with Shared Attention and Manipulation",
        "type": "poster"
    },
    "upp1043": {
        "abstract": "Aalto Interface Metrics (AIM) pools several empirically validated models and metrics of user perception and attention into an easy-to-use online service for the evaluation of graphical user interface (GUI) designs. Users input a GUI design via URL, and select from a list of 17 different metrics covering aspects ranging from visual clutter to visual learnability. AIM presents detailed breakdowns, visualizations, and statistical comparisons, enabling designers and practitioners to detect shortcomings and possible improvements. The web service and code repository are available at interfacemetrics.aalto.fi.",
        "authors": [
            {
                "name": "Antti  Oulasvirta, Aalto University"
            },
            {
                "name": "Samuli  De Pascale, Aalto University"
            },
            {
                "name": "Janin  Koch, Aalto University"
            },
            {
                "name": "Thomas  Langerak, Aalto University"
            },
            {
                "name": "Jussi  Jokinen, Aalto University"
            },
            {
                "name": "Kashyap  Todi, Aalto University"
            },
            {
                "name": "Markku  Laine, Aalto University"
            },
            {
                "name": "Manoj  Kristhombuge, Aalto University"
            },
            {
                "name": "Yuxi  Zhu, Aalto University"
            },
            {
                "name": "Aliaksei  Miniukovich, University of Trento"
            },
            {
                "name": "Gregorio  Palmas, KTH Royal Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "user interfaces",
            "metrics",
            "computational evaluation",
            "UI layouts",
            ""
        ],
        "subtype": "poster",
        "title": "Aalto Interface Metrics (AIM): A Service and Codebase for Computational GUI Evaluation",
        "type": "poster"
    },
    "upp1044": {
        "abstract": "Across many domains, interactive systems either make decisions for us autonomously or yield decision-making authority to us and play a supporting role. However, many settings, such as those in education or the workplace, benefit from sharing this autonomy between the user and the system, and thus from a system that adapts to them over time. In this paper, we pursue two primary research questions: (1) How do we design interfaces to share autonomy between the user and the system? (2) How does shared autonomy alter a user\"s perception of a system? We present SharedKeys, an interactive shared autonomy system for piano instruction that plays different video segments of a piece for students to emulate and practice. Underlying our approach to shared autonomy is a mixed-observability Markov decision process that estimates a user\"s desired autonomy level based on her performance and attentiveness. Pilot studies revealed that students sharing autonomy with the system learned more quickly and perceived the system as more intelligent. ",
        "authors": [
            {
                "name": "Sharon  Zhou, Stanford University"
            },
            {
                "name": "Tong  Mu, Stanford University"
            },
            {
                "name": "Karan  Goel, Stanford University"
            },
            {
                "name": "Michael  Bernstein, Stanford University"
            },
            {
                "name": "Emma  Brunskill, Stanford University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "poster",
        "title": "Shared Autonomy for Interactive Systems",
        "type": "poster"
    },
    "upp1046": {
        "abstract": "Presentation slides play an important role in online lecture videos. Slides convey the main points of the lecture visually, while the instructor's narration adds detailed verbal explanations to each item in the slide. We call the link between a slide item and the corresponding part of the narration a reference. In order to assess the feasibility of reference-based interaction techniques for watching videos, we introduce DynamicSlide, a video processing system that automatically extracts references from slide-based lecture videos and a video player. The system incorporates a set of reference-based techniques: emphasizing the current item in the slide that is being explained, enabling item-based navigation, and enabling item-based note-taking. Our pipeline correctly finds 79% of the references in a set of five videos with 141 references. Results from a user study suggest that DynamicSlide's features improve the learner's video browsing and navigation experience.",
        "authors": [
            {
                "name": "Hyeungshik  Jung, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Hijung Valentina  Shin, Adobe Research"
            },
            {
                "name": "Juho  Kim, Korea Advanced Institute of Science & Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Educational videos",
            "/Visual navigation",
            "Video learning"
        ],
        "subtype": "poster",
        "title": "DynamicSlide: Reference-based Interaction Techniques for Slide-based Lecture Videos",
        "type": "poster"
    },
    "upp1049": {
        "abstract": "Despite advances in machine learning and deep neural networks, there is still a huge gap between machine and human image understanding. One of the causes is the annotation process used to label training images. In most image categorization tasks, there is a fundamental ambiguity between some image categories and the underlying class probability differs from very obvious cases to ambiguous ones. However, current machine learning systems and applications usually work with discrete annotation processes and the training labels do not reflect this ambiguity. To address this issue, we propose an new image annotation framework where labeling incorporates human gaze behavior. In this framework, gaze behavior is used to predict image labeling difficulty. The image classifier is then trained with sample weights defined by the predicted difficulty. We demonstrate our approach's effectiveness on four-class image classification tasks.",
        "authors": [
            {
                "name": "Tatsuya  Ishibashi, Osaka University"
            },
            {
                "name": "Yusuke  Sugano, Osaka University"
            },
            {
                "name": "Yasuyuki  Matsushita, Osaka University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Eye tracking",
            "Machine learning",
            "Computer vision"
        ],
        "subtype": "poster",
        "title": "Gaze-guided Image Classification for Reflecting Perceptual Class Ambiguity",
        "type": "poster"
    },
    "upp1052": {
        "abstract": "We present Touch180, a computer vision based solution for identifying fingers on a mobile touchscreen with a fisheye camera and deep learning algorithm. As a proof-of-concept research, this paper focused on robustness and high accuracy of finger identification. We generated a new dataset for Touch180 configuration, which is named as Fisheye180. We trained a CNN (Convolutional Neural Network)-based network utilizing touch locations as auxiliary inputs. With our novel dataset and deep learning algorithm, finger identification result shows 98.56% accuracy with VGG16 model. Our study will serve as a step stone for finger identification on a mobile touchscreen.",
        "authors": [
            {
                "name": "Insu  Kim, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Keunwoo  Park, Korea Advanced Institute of Science & Technology"
            },
            {
                "name": "Youngwoo  Yoon, Korea Advanced Institute of Science and Technology & ETRI"
            },
            {
                "name": "Geehyuk  Lee, Korea Advanced Institute of Science & Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Touchscreen",
            "Finger Identification",
            "Fisheye Camera",
            "Mobile",
            "Smart Device",
            "Deep Learning",
            "Convolutional Neural Network"
        ],
        "subtype": "poster",
        "title": "Touch180: Finger Identification on Mobile Touchscreen using Fisheye Camera and Convolutional Neural Network",
        "type": "poster"
    },
    "upp1054": {
        "abstract": "We propose OmniEyeball (OEB), which is a novel interactive 360° image I/O system. It integrates the spherical display system with an omnidirectional camera to enable both capturing the 360° panoramic live streaming video as well as displaying it. We also present its unique application for symmetric 360° video communication by utilizing two OEB terminals, which may solve the narrow field-of-view problem in video communication. In addition, we designed a vision-based touch detection technique as well as some features to support 360° video communication. ",
        "authors": [
            {
                "name": "Zhengqing  Li, Tokyo Institute of Technology"
            },
            {
                "name": "Shio  Miyafuji, Tokyo Institute of Technology"
            },
            {
                "name": "Toshiki  Sato, Tokyo Institute of Technology"
            },
            {
                "name": "Hideaki  Kuzuoka, University of Tsukuba"
            },
            {
                "name": "Hideki  Koike, Tokyo Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Spherical display",
            "omnidirectional camera",
            "360-degree panorama",
            "video communication"
        ],
        "subtype": "poster",
        "title": "OmniEyeball: Spherical Display Equipped With Omnidirectional Camera And Its Application For 360-Degree Video Communication",
        "type": "poster"
    },
    "upp1055": {
        "abstract": "We propose a technique to support writing activity in a confidential manner with a pen-based device. Autocorrect and predictive conversion do not work when writing by hand, and looking up unknown spelling is sometimes embarrassing. Therefore, we propose AmbientLetter which seamlessly and discretely presents the forgotten spelling to the user in scenarios where handwriting is necessary. In this work, we describe the system structure and the technique used to conceal the user\"s getting the information.",
        "authors": [
            {
                "name": "Xaver Tomihiro Toyozaki, Meiji University"
            },
            {
                "name": "Keita  Watanabe, Meiji University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Subtle interaction",
            "Ubiquitous computing"
        ],
        "subtype": "poster",
        "title": "AmbientLetter: Letter Presentation Method for Discreet Notification of Unknown Spelling when Handwriting",
        "type": "poster"
    },
    "upp1058": {
        "abstract": "Vibrations generated by human activity have been used for recognizing human behavior and developing user interfaces; however, it is difficult to estimate static poses that do not generate a vibration. This can be solved using active acoustic sensing; however, this method is not suitable for emitting some vibrations around the head in terms of the influence of audition. Therefore, we propose a method for estimating head poses using body-conducted sound naturally and regularly generated in the human body. The support vector classification recognizes vertical and horizontal directions of the head, and we confirmed the feasibility of the proposed method through experiments.",
        "authors": [
            {
                "name": "Ryo  Kamoshida, Tokai University"
            },
            {
                "name": "Kentaro  Takemura, Tokai University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Body-conducted sound",
            "head pose",
            "vibrations",
            "frequency bands"
        ],
        "subtype": "poster",
        "title": "Head Pose Classification by using Body-Conducted Sound",
        "type": "poster"
    },
    "upp1059": {
        "abstract": "Symptoms of progressing dementia like memory loss, impaired executive function and decreasing motivation can gradually undermine instrumental activities of daily living (IADL) such as cooking.  Assisting technologies in form of augmented reality (AR) have previously been applied to support cognitively impaired users during IADLs.  In most cases, instructions were provided locally via projection or a head-mounted display (HMD) but lacked an incentive mechanism and the flexibility to support a broad range of use-cases. To provide users and therapists with a holistic solution, we propose cARe, a framework that can be easily adapted by therapists to various use-cases without any programming knowledge.  Users are then guided through manual processes with localized visual and auditory cues that are rendered by an HMD. Our ongoing user study indicates that users are more comfortable and successful in cooking with cARe as compared to a printed recipe, which promises a more dignified and autonomous living for dementia patients.",
        "authors": [
            {
                "name": "Dennis  Wolf, Ulm University"
            },
            {
                "name": "Daniel  Besserer, Ulm University"
            },
            {
                "name": "Karolina  Sejunaite, Ulm University"
            },
            {
                "name": "Matthias Wilhelm Riepe, Ulm University"
            },
            {
                "name": "Enrico  Rukzio, Ulm University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "dementia",
            "augmented reality",
            "support",
            "autonomy"
        ],
        "subtype": "poster",
        "title": "cARe: An Augmented Reality Support System for Dementia Patients",
        "type": "poster"
    },
    "upp1060": {
        "abstract": "Research has been done in sophisticated notifications, still, devices today mainly stick to a binary level of information, while they are either attention drawing or silent. We propose scalable notifications, which adjust the intensity level reaching from subtle to obtrusive and even going beyond that level while forcing the user to take action. To illustrate the technical feasibility and validity of this concept, we developed three prototypes. The prototypes provided mechano-pressure, thermal, and electrical feedback, which were evaluated in different lab studies. Our first prototype provides subtle poking through to high and frequent pressure on the user’s spine, which significantly improves back posture. In a second scenario, the user is able to perceive the overuse of a drill by an increased temperature on the palm of a hand until the heat is intolerable, forcing the user to eventually put down the tool. The last application comprises of a speed control in a driving simulation, while electric muscle stimulation on the users’ legs, conveys information on changing the car’s speed by a perceived tingling until the system forces the foot to move involuntarily. In conclusion, all studies’ findings support the feasibility of our concept of a scalable notification system, including the system forcing an intervention.",
        "authors": [
            {
                "name": "Denys J.C. Matthies, Auckland Bioengineering Institute & Fraunhofer IGD Rostock"
            },
            {
                "name": "Laura Milena  Daza Parra, Fraunhofer IGD Rostock"
            },
            {
                "name": "Bodo  Urban, Fraunhofer IGD Rostock"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Scaling notifications",
            "electrical muscle stimulation",
            "thermal feedback",
            "haptic feedback",
            "wearable prototyping"
        ],
        "subtype": "poster",
        "title": "Scaling Notifications Beyond Alerts: From Subtly Drawing Attention up to Forcing the User to Take Action",
        "type": "poster"
    },
    "upp1066": {
        "abstract": "We present Companion, a software tool tailored towards improving and digitally supporting the pen-and-paper tabletop role-playing experience. Pen-and-paper role-playing games (P&P RPG) are a concept known since the early 1970s. Since then, the genre has attracted a massive community of players while branching out into several genres and P&P RPG systems to choose from. Due to the highly interactive and dynamic nature of the game, a participants individual impact on narrative and interactive aspects of the game is extremely high. The diversity of scenarios within this context unfold a variety of players needs, as well as factors limiting and enhancing game-play. Companion offers an audio management workspace for creation and playback of soundscapes based on visual layouting. It supports interactive image presentation and map exploration which can incorporate input from any device providing TUIO tracking data. Additionally, a mobile app was developed to be used as a remote control for media activation on the desktop host. ",
        "authors": [
            {
                "name": "Sebastian  Stickert, Bauhaus-Universität Weimar"
            },
            {
                "name": "Hagen  Hiller, Bauhaus-Universität Weimar"
            },
            {
                "name": "Florian  Echtler, Bauhaus-Universität Weimar"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Pen-and-paper role-playing games",
            "Soundscaping",
            "Image Interaction",
            "Tangible Interaction"
        ],
        "subtype": "poster",
        "title": "Companion - A Software Toolkit for Digitally Aided Pen-and-Paper Tabletop Roleplaying",
        "type": "poster"
    },
    "upp1071": {
        "abstract": "The literate programming paradigm presents a program interleaved with natural language text explaining the code's rationale and logic.  While this is great for program readers, the labor of creating literate programs deters most program authors from providing this text at authoring time. Instead, as we determine through interviews, developers provide their design rationales after the fact, in discussions with collaborators.  We propose to capture these discussions and incorporate them into the code. We have prototyped a tool to link online discussion of code directly to the code it discusses. Incorporating these discussions incrementally creates post-literate programs that convey information to future developers.",
        "authors": [
            {
                "name": "Soya  Park, Massacshuetts Institute of Technology"
            },
            {
                "name": "Amy X. Zhang, Massachusetts Institute of Technology"
            },
            {
                "name": "David R. Karger, Massachusetts Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "software development",
            "group chat",
            "knowledge management"
        ],
        "subtype": "poster",
        "title": "Post-literate Programming: Linking Discussion and Code in Software Development Teams",
        "type": "poster"
    },
    "upp1073": {
        "abstract": "Learning new motor skills is a problem that people are constantly confronted with (e.g. to learn a new kind of sport). In our work, we investigate to which extent the learning\nprocess of a motor sequence can be optimized with the help of Augmented Reality as a technical assistant. Therefore, we propose an approach that divides the problem into three tasks: (1) the tracking of the necessary movements, (2) the creation of a model that calculates possible deviations and (3) the implementation of a visual feedback system. To evaluate our approach, we implemented the idea by using infrared depth sensors and an Augmented Reality head-mounted device (HoloLens). Our results show that the system can give an efficient assistance for the correct height of a throw with one ball. Furthermore, it provides a basis for the support of a complete juggling sequence.",
        "authors": [
            {
                "name": "Benjamin  Meyer, OFFIS - Institute for Information Technology"
            },
            {
                "name": "Pascal  Gruppe, University of Oldenburg"
            },
            {
                "name": "Bastian  Cornelsen, University of Oldenburg"
            },
            {
                "name": "Tim Claudius  Stratmann, University of Oldenburg"
            },
            {
                "name": "Uwe  Gruenefeld, University of Oldenburg"
            },
            {
                "name": "Susanne  Boll, University of Oldenburg"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Augmented Reality",
            "Tracking",
            "Physical Model",
            "Interaction",
            "Motor Skills",
            "Juggling"
        ],
        "subtype": "poster",
        "title": "Juggling 4.0: Learning Complex Motor Skills with Augmented Reality Through the Example of Juggling",
        "type": "poster"
    },
    "upp1074": {
        "abstract": "In this paper, we present a wearable kinesthetic I/O device, which is able to measure and intervene in multiple muscle activities simultaneously through the same electrodes. \nThe developed system includes an I/O module, capable of measuring the electromyogram (EMG) of four muscle tissues, while applying electrical muscle stimulation (EMS) at the same time.\nThe developed wearable system is configured in a scalable manner for achieving 1) high stimulus frequency (up to 70 Hz), 2) wearable dimensions in which the device can be placed along the limbs, and 3) flexibility of the number of I/O electrodes (up to 32 channels).\nIn a pilot user study, which shared the wrist compliance between two persons, participants were able to recognize the level of their confederate's wrist joint compliance using a 4-point Likert scale.\nThe developed system would benefit a physical therapist and a patient, during hand rehabilitation, using a peg board for sharing their wrist compliance and grip force, which are usually difficult to be observed in a visual contact.",
        "authors": [
            {
                "name": "Jun  Nishida, University of Tsukuba"
            },
            {
                "name": "Kenji  Suzuki, University of Tsukuba"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Wearable Device",
            "EMS",
            "EMG",
            "Joint Compliance"
        ],
        "subtype": "poster",
        "title": "Wearable Kinesthetic I/O Device for Sharing Muscle Compliance",
        "type": "poster"
    },
    "upp1075": {
        "abstract": "Biased perceptions of others are known to negatively influence the outcomes of social and professional interactions in many regards. Theses biases can be informed by a multitude of non-verbal cues such as voice pitch and voice volume. This project explores how haptic effects, generated from speech, could attenuate listeners' perceived voice-related biases formed from a speaker's voice pitch. Promising preliminary results collected during a decision-making task suggest that the speech to haptic mapping and vibration delivery mechanism employed does attenuate voice-related biases. Accordingly, it is anticipated that such a system could be introduced in the workplace to equalize people's contribution opportunities and to create a more inclusive environment by reversing voice-related biases.",
        "authors": [
            {
                "name": "Feras  Al Taha, McGill University"
            },
            {
                "name": "Pascal E. Fortin, McGill University"
            },
            {
                "name": "Antoine  Weill-Duflos, McGill University"
            },
            {
                "name": "Jeremy  Cooperstock, McGill University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Speech",
            "Haptics",
            "Social Computing",
            "Affective Computing"
        ],
        "subtype": "poster",
        "title": "Reversing Voice-Related Biases Through Haptic Reinforcement",
        "type": "poster"
    },
    "upp1077": {
        "abstract": "In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators’ perspectives on the object as well as understand one another’s perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other’s perspective; (2) explore objects independently from one another, and (3) render gestures in the remote’s workspace. In this work, we focus on the expert’s role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.",
        "authors": [
            {
                "name": "Martin  Feick, htw saar"
            },
            {
                "name": "Tony  Tang, University of Calgary"
            },
            {
                "name": "Scott  Bateman, University of New Brunswick"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Mixed Reality",
            "Object-Focused Remote Collaboration",
            "AR",
            "VR"
        ],
        "subtype": "poster",
        "title": "Mixed-Reality for Object-Focused Remote Collaboration",
        "type": "poster"
    },
    "upp1080": {
        "abstract": "In this paper, we present a novel telexistence system and design methods for telexistence studies to explore spatialscale deconstruction. There have been studies on the experience of dwarf-sized or giant-sized telepresence have been conducted over a period of many years. In this study, we discuss the scale of movements, image transformation, technical components of telepresence robots, and user experiences of telexistence-based spatial transformations. We implemented two types of telepresence robots with an omnidirectional stereo camera setup for a spatial trans-scale experience, wheeled robots, and quadcopters. These telepresence robots provide users with a trans-scale experience for a distance ranging from 15 cm to 30 m. We conducted user studies for different camera positions on robots and for different image transformation method.",
        "authors": [
            {
                "name": "Satoshi  Hashizume, University of Tsukuba"
            },
            {
                "name": "Akira  Ishii, University of Tsukuba"
            },
            {
                "name": "Kenta  Suzuki, University of Tsukuba"
            },
            {
                "name": "Kazuki  Takazawa, University of Tsukuba"
            },
            {
                "name": "Yoichi  Ochiai, University of Tsukuba"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Telepresence",
            "telexistence",
            "omnidirectional image",
            "immersive experience",
            "out-of-body"
        ],
        "subtype": "poster",
        "title": "Trans-scale Playground: An Immersive Visual Telexistence System for Human Adaptation",
        "type": "poster"
    },
    "upp1091": {
        "abstract": "To filter and shut out an increasingly loud environment, many resort to the use of personal audio technology. They drown out unwanted sounds, by wearing headphones. This uniform interaction with all surrounding sounds can have a negative impact on social relations and situational awareness. Leveraging mediation through smarter headphones, users gain more agency over their sense of hearing: For instance by being able to selectively alter the volume and other features of specific sounds, without losing the ability to add media.  In this work, we propose the vision of interactive auditory mediated reality (AMR). To understand users' attitude and requirements, we conducted a week-long event sampling study (n = 12), where users recorded and rated sources (n = 225) which they would like to mute, amplify or turn down. The results indicate that besides muting, a distinct, \"quiet-but-audible\"volume exists. It caters to two requirements at the same time: aesthetics/comfort and information acquisition.",
        "authors": [
            {
                "name": "Evgeny  Stemasov, Ulm University"
            },
            {
                "name": "Gabriel  Haas, Ulm University"
            },
            {
                "name": "Michael  Rietzler, Ulm University"
            },
            {
                "name": "Enrico  Rukzio, Ulm University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Auditory Mediated Reality",
            "Hearables",
            "Augmented Hearing"
        ],
        "subtype": "poster",
        "title": "Augmenting Human Hearing Through Interactive Auditory Mediated Reality",
        "type": "poster"
    },
    "upp1094": {
        "abstract": "User interface software and technologies have been evolving significantly and rapidly. This poster presents a breakthrough user experience that leverages multisensorial priming and embedded interaction and introduces an interactive piece of furniture called Sense.Seat. Sensory stimuli such as calm colors, lavender and other scents as well as ambient soundscapes have been traditionally used to spark creativity and promote well-being. Sense.Seat is the first computational multisensorial seat that can be digitally controlled and vary the frequency and intensity of visual, auditory and olfactory stimulus. It is a new user interface shaped as a seat or pod that primes the user for inducing improved mood and cognition, therefore improving the work environment.",
        "authors": [
            {
                "name": "Pedro F. Campos, Madeira-ITI, University of Madeira"
            },
            {
                "name": "Diogo  Cabral, Madeira-ITI, University of Madeira"
            },
            {
                "name": "Frederica  Gonçalves, Madeira-ITI, University of Madeira"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Multisensorial",
            "priming",
            "interactive furniture",
            "ergonomics",
            "input and output",
            "interaction design",
            "user experience"
        ],
        "subtype": "poster",
        "title": "Sense.Seat: Inducing Improved Mood and Cognition through Multisensorial Priming",
        "type": "poster"
    },
    "upp1095": {
        "abstract": "Evaluating and selecting ideas is a critical and time-consuming step in collaborative ideation, making computational support for this task a desired research goal. However, existing automatic approaches to idea selection might eliminate valuable ideas.\nIn this work we combine automatic approaches with human sensemaking. Kaleidoscope is an exploratory data analytics tool based on semantic technologies. It supports users in exploring and annotating existing ideas interactively. In the following, we present key design principles of Kaleidoscope. Based on qualitative feedback collected on a prototype, we identify potential improvements and describe future work.",
        "authors": [
            {
                "name": "Maximilian  Mackeprang, Freie Universität Berlin"
            },
            {
                "name": "Philipp  Kuhnz, Freie Universität Berlin"
            },
            {
                "name": "Gerold  Schneider, Freie Universität Berlin"
            },
            {
                "name": "Johann  Strama, Freie Universität Berlin"
            },
            {
                "name": "Jesse Josua Benjamin, Freie Universität Berlin"
            },
            {
                "name": "Claudia  Müller-Birn, Freie Universität Berlin"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Information Visualization",
            "Evaluation",
            "Collaborative Ideation"
        ],
        "subtype": "poster",
        "title": "Kaleidoscope: An RDF-based Exploratory Data Analysis Tool for Ideation Outcomes",
        "type": "poster"
    },
    "upp1096": {
        "abstract": "One of the main drawbacks of the fixation-based gaze interfaces is that they are unable to distinguish top-down attention (or selection, a gaze with a purpose) from stimulus driven bottom-up attention (or navigation, a stare without any intentions) without time durations or unnatural eye movements. We found that using the bistable image called the Necker's cube as a button user interface (UI) helps to remedy the limitation. When users switch two rivaling percepts of the Necker's cube at will, unique eye movements are triggered and these characteristics can be used to indicate a button press or a selecting action. In this paper, we introduce (1) the cognitive phenomenon called \"percept switch\"for gaze interaction, and (2) propose \"perceptual switch\"or the Necker's cube user interface (UI) which uses \"percept switch\"as the indication of a selection. Our preliminary experiment confirms that perceptual switch can be used to distinguish voluntary gaze selection from random navigation, and discusses that the visual elements of the Necker's cube such as size and biased visual cues could be adjusted for the optimal use of individual users.",
        "authors": [
            {
                "name": "Jooyeon  Lee, Yonsei University"
            },
            {
                "name": "Jong-Seok  Lee, Yonsei University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Gaze Interaction",
            "Selection User Interface",
            "Necker’s Cube",
            "Ambiguous Illusion",
            "Bistable Image",
            "Percept Switch"
        ],
        "subtype": "poster",
        "title": "Perceptual Switch for Gaze Selection",
        "type": "poster"
    },
    "upp1097": {
        "abstract": "ZEUSSS (Zero Energy Ubiquitous Sound Sensing Surface), allows physical objects and surfaces to be instrumented with a thin, self-sustainable material that provides acoustic sensing and communication capabilities. We have built a prototype ZEUSSS tag using minimal hardware and flexible electronic components, extending our original self-sustaining SATURN microphone with a printed, flexible antenna to support passive communication via analog backscatter. ZEUSSS enables objects to have ubiquitous wire-free battery-free audio based context sensing, interaction, and surveillance capabilities.",
        "authors": [
            {
                "name": "Nivedita  Arora, Georgia Institute of Technology"
            },
            {
                "name": "Gregory D. Abowd, Georgia Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Battery-free Microphone",
            "Analog Backscatter Communication",
            "TENG (Triboelectric Nanogenerator)",
            "Flexible electronics"
        ],
        "subtype": "poster",
        "title": "ZEUSSS: Zero Energy Ubiquitous Sound Sensing Surface Leveraging Triboelectric Nanogenerator and Analog Backscatter Communication",
        "type": "poster"
    },
    "upp1104": {
        "abstract": "We present a tangible memory notebook--reMi--that records the ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device's 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall and share a memory, we investigate how tangible motion of a paper that represents sound can enhance the \"reminiscence\".",
        "authors": [
            {
                "name": "Kyung Yun  Choi, Massacshuetts Institute of Technology"
            },
            {
                "name": "Darle  Shinsato, Massachusetts Institute of Technology"
            },
            {
                "name": "Shane  Zhang, Massachusetts Institute of Technology"
            },
            {
                "name": "Ken  Nakagaki, Massachusetts Institute of Technology"
            },
            {
                "name": "Hiroshi  Ishii, Massachusetts Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Tangible Memory",
            "Shape-changing Paper",
            "Reminisce"
        ],
        "subtype": "poster",
        "title": "reMi: Translating Ambient Sounds of Moment into Tangible and Shareable Memories through Animated Paper",
        "type": "poster"
    },
    "upp1106": {
        "abstract": "Most artificial intelligence (AI) systems to date have focused entirely on performance, and rarely if at all on their social interactions with people and how to balance the AIs\"goals against their human collaborators\". Learning quickly from interactions with people poses both social challenges and is unresolved technically. In this paper, we introduce engagement learning: a training approach that learns to trade off what the AI needs---the knowledge value of a label to the AI---against what people are interested to engage with---the engagement value of the label. We realize our goal with ELIA (Engagement Learning Interaction Agent), a conversational AI agent who\"s goal is to learn new facts about the visual world by asking engaging questions of people about the photos they upload to social media. Our current deployment of ELIA on Instagram receives a response rate of 26%.",
        "authors": [
            {
                "name": "Ranjay  Krishna, Stanford University"
            },
            {
                "name": "Donsuk  Lee, Stanford University"
            },
            {
                "name": "Fei-Fei  Li, Stanford University"
            },
            {
                "name": "Michael  Bernstein, Stanford University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "engagement learning",
            "reinforcement learning",
            "scene understanding",
            "computer vision",
            "natural language generation"
        ],
        "subtype": "poster",
        "title": "Engagement Learning: Expanding Visual Knowledge by Engaging Online Participants",
        "type": "poster"
    },
    "upp1107": {
        "abstract": "We describe the development and user testing of an ambient display for autonomous vehicles. Instead of providing feedback about driving actions, once executed, it communicates driving decisions in advance, via light signals in passengers\"peripheral vision. This ambient display was tested in an WoZ-based on-the-road-driving simulation of a fully autonomous vehicle.  Findings from a preliminary study with 14 participants suggest that such a display might be particularly useful to communicate upcoming inertia changes for passengers.",
        "authors": [
            {
                "name": "Hauke  Sandhaus, Bauhaus-Universität Weimar"
            },
            {
                "name": "Eva  Hornecker, Bauhaus-Universität Weimar"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Autonomous Vehicle Interfaces",
            "Methodology",
            "On-Road simulation",
            "Ambient Display",
            ""
        ],
        "subtype": "poster",
        "title": "A WOZ Study of Feedforward Information on an Ambient Display in Autonomous Cars",
        "type": "poster"
    },
    "upp1117": {
        "abstract": "Online crowds, with their large numbers and diversity, show great potential for creativity, particularly during large-scale brainstorming sessions. Research has explored different ways of augmenting this creativity, such as showing ideators some form of inspiration to get them to explore more categories or generate more ideas. The mechanisms used to select which inspirations are shown to ideators thus far have been focused on characteristics of the inspirations rather than on ideators. This can hinder their effect, as creativity research has shown that ideators have unique cognitive structures and may therefore be better inspired by some ideas rather than others. We introduce CrowdMuse, an adaptive system for supporting large scale brainstorming. The system models ideators based on their past ideas and adapts the system views and inspiration mechanisms accordingly. An evaluation of this system could inform how to better individually support ideators.",
        "authors": [
            {
                "name": "Victor  Girotto, Arizona State University"
            },
            {
                "name": "Erin  Walker, Arizona State University"
            },
            {
                "name": "Winslow  Burleson, New York University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Creativity",
            "brainstorming",
            "crowd",
            "adaptive systems."
        ],
        "subtype": "poster",
        "title": "CrowdMuse: An Adaptive Crowd Brainstorming System",
        "type": "poster"
    },
    "upp1122": {
        "abstract": "Smartphone user authentication is still an open challenge because the balance between both security and usability is indispensable. To balance between them, active authentication is one way to overcome the problem. In this paper, we tackle to improve the accuracy of active authentication by adopting online learning with touch pressure. In recent years, it becomes easy to use the smartphones equipped with pressure sensor so that we have confirmed the effectiveness of adopting the touch pressure as one of the features to authenticate. Our experiments adopting online AROW algorithm with touch pressure show that equal error rate (EER), where the miss rate and false rate are equal, is reduced up to one-fifth by adding touch pressure feature. Moreover, we have confirmed that training with the data from both sitting posture and prone posture archives the best when testing variety of postures including sitting, standing and prone, which achieves EER up to 0.14%.",
        "authors": [
            {
                "name": "Masashi  Kudo, Waseda University"
            },
            {
                "name": "Hayato  Yamana, Waseda University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Active authentication",
            "Online learning",
            "Touch pressure"
        ],
        "subtype": "poster",
        "title": "Active Authentication on Smartphone using Touch Pressure",
        "type": "poster"
    },
    "upp1124": {
        "abstract": "We introduce DisplayBowl which is a concept of a bowl shaped hemispherical display for showing omnidirectional images. This display provides three-way observation for omnidirectional images. DisplayBowl allows users to observe an omnidirectional image by looking the image from above. In addition, users can see it with a first-person-viewpoint, by looking into the inside of the hemispherical surface from diagonally above. Furthermore, by observing both the inside and the outside of the hemispherical surface at the same time from obliquely above, it is possible to observe it by a pseudo third-person-viewpoint, like watching the drone obliquely from behind. These ways of viewing solve the problem of inability of pilots controlling a remote vehicle such as a drone to notice what happens behind them, which happen with conventional displays such as flat displays and head mounted displays.",
        "authors": [
            {
                "name": "Shio  Miyafuji, Tokyo Institute of Technology"
            },
            {
                "name": "Soichiro  Toyohara, Tokyo Institute of Technology"
            },
            {
                "name": "Toshiki  Sato, Tokyo Institute of Technology"
            },
            {
                "name": "Hideki  Koike, Tokyo Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Spherical display",
            "Entertainment",
            "Remote control"
        ],
        "subtype": "poster",
        "title": "DisplayBowl: A Bowl-Shaped Display for Omnidirectional Videos",
        "type": "poster"
    },
    "upp1126": {
        "abstract": "Can natural interaction requirements be fulfilled while still harnessing the \"supernatural\"fantasy of Virtual Reality (VR)? In this work we used off the shelf Electromyogram (EMG) sensors as an input device which can afford natural gestures to preform the \"supernatural\"task of growing your arm in VR. We recorded 18 participants preforming a simple retrieval task in two phases; an initial and a learning phase where the stretch arm was disabled and enabled respectively. The results show that the gestures used in the initial phase are different than the main gestures used to retrieve an object in our system and that the times taken to complete the learning phase are highly variable across participants. ",
        "authors": [
            {
                "name": "Chloe  Eghtebas, Technical University of Munich"
            },
            {
                "name": "Sandro  Weber, Technical University of Munich"
            },
            {
                "name": "Gudrun  Klinker, Technical University of Munich"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Natural Interaction",
            "EMG",
            "VR"
        ],
        "subtype": "poster",
        "title": "Investigation into Natural Gestures Using EMG for SuperNatural\"Interaction in VR\"",
        "type": "poster"
    },
    "upp1127": {
        "abstract": "In daily communications, we often use interpersonal cues - telltale facial expressions and body language - to moderate responses to our conversation partners. While we are able to interpret gaze as a sign of interest or reluctance, conventional user interfaces do not yet possess this possible benefit. In our work, we evaluate to what degree fixation-based gaze metrics can be used to infer a user's personal interest in the displayed content. We report on a study (N=18) where participants were presented with a grid array of different images, whilst being recorded for gaze behavior. Our system calculated a ranking for shown images based on gaze metrics. We found that all metrics are effective indicators of the participants' interest by analyzing their agreement with regard to the system's ranking. In an evaluation in a museum, we found that this translates to in-the-wild scenarios despite environmental constraints, such as limited data accuracy.",
        "authors": [
            {
                "name": "Jakob  Karolus, LMU Munich"
            },
            {
                "name": "Patrick  Dabbert, University of Stuttgart"
            },
            {
                "name": "Pawel W. Wozniak, University of Stuttgart"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Eye-tracking",
            "gaze metrics",
            "personal interest,Eye-tracking",
            "gaze metrics",
            "personal interest."
        ],
        "subtype": "poster",
        "title": "I Know What You Want: Using Gaze Metrics to Predict Personal Interest",
        "type": "poster"
    },
    "upp1131": {
        "abstract": "Patients waiting for long to use medical services become more physically and psychologically anxious than do people waiting to use general services. Since children feel more anxiety and fear in a hospital, it is necessary to reduce their perceived waiting time by disturbing their awareness of time and dispersing their attention.\nWe present the D-Aquarium, a computer-based digital aquarium that provides psychological stability to pediatric patients and reduces their perceived waiting time by using distractions to alleviate their psychological anxiety and interfere with their perception of time.",
        "authors": [
            {
                "name": "Jooyoung  Son, Sungkyunkwan University"
            },
            {
                "name": "Suzi  Choi, Sungkyunkwan University"
            },
            {
                "name": "Jundong  Cho, Sungkyunkwan University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Children’s hospital",
            "Perceived waiting time",
            "Distraction from time",
            "Digital aquarium",
            "Waiting space"
        ],
        "subtype": "poster",
        "title": "D-Aquarium: A Digital Aquarium to Reduce Perceived Waiting Time at Children’s Hospital",
        "type": "poster"
    },
    "upp1132": {
        "abstract": "Physical buttons provide rich force characteristics during the travel range, which are commonly described in the form of force-displacement curves. These force characteristics play an important role in the users' experiences while pressing a button. However, due to lack of proper tools to dynamically render various force-displacement curves, little literature has tried iterative button design improvement. This paper presents Button Simulator, a low-cost 3D printed physical button capable of displaying any force-displacement curves, with limited average error offset around .034 N. By reading the force-displacement curves of existing push-buttons, we can easily replicate the force characteristics from any buttons onto our Button Simulator. One can even go beyond existing buttons and design non-existent ones as the form of arbitrary force-displacement curves; then use Button Simulator to render the sensation. This project will be open-sourced and the implementation details will be released. Our system can be a useful tool for future researchers, designers, and makers to investigate rich and dynamic button\"s force design.",
        "authors": [
            {
                "name": "Yi-Chi  Liao, Aalto University"
            },
            {
                "name": "Sunjun  Kim, Aalto University"
            },
            {
                "name": "Antti  Oulasvirta, Aalto University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Button",
            "input devices",
            "force-displacement curve",
            "input engineering."
        ],
        "subtype": "poster",
        "title": "One Button to Rule Them All: Rendering Arbitrary Force-Displacement Curves",
        "type": "poster"
    },
    "upp1135": {
        "abstract": "Eye tracking is expected to become an integral part of future augmented reality (AR) head-mounted displays (HMDs) given that it can easily be integrated into existing hardware and provides a versatile interaction modality. To augment objects in the real world, AR HMDs require a three-dimensional understanding of the scene, which is currently solved using depth cameras. In this work we aim to explore how 3D gaze data can be used to enhance scene understanding for AR HMDs by envisioning a symbiotic human-machine depth camera, fusing depth data with 3D gaze information. We present a first proof of concept, exploring to what extend we are able to recognise what a user is looking at by plotting 3D gaze data. To measure 3D gaze, we implemented a vergence-based algorithm and built an eye tracking setup consisting of a Pupil Labs headset and an OptiTrack motion capture system, allowing us to measure 3D gaze inside a 50x50x50 cm volume. We show first 3D gaze plots of \"gazed-at\"objects and describe our vision of a symbiotic human-machine depth camera that combines a depth camera and human 3D gaze information.",
        "authors": [
            {
                "name": "Teresa  Hirzle, Ulm University"
            },
            {
                "name": "Jan  Gugenheimer, Ulm University"
            },
            {
                "name": "Florian  Geiselhart, Ulm University"
            },
            {
                "name": "Andreas  Bulling, University of Stuttgart"
            },
            {
                "name": "Enrico  Rukzio, Ulm University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "3D gaze",
            "eye-based interaction",
            "human-machine symbiosis"
        ],
        "subtype": "poster",
        "title": "Towards a Symbiotic Human-Machine Depth Sensor: Exploring 3D Gaze for Object Reconstruction",
        "type": "poster"
    },
    "upp1136": {
        "abstract": "In this paper, we developed an auditory display method which improves the comprehension of photograph to apply the support system for person with visual impairment. The auralization method is constructed by object recognition, auditory iconization and stereophonic techniques. Through the experiments, the enhancement of intelligibility and discriminability was confirmed compared to the image-to-speech reading machine method.",
        "authors": [
            {
                "name": "Keichi  Zempo, University of Tsukuba"
            },
            {
                "name": "Yuichi  Mashiba, University of Tsukuba"
            },
            {
                "name": "Takayuki  Kawamura, University of Tsukuba"
            },
            {
                "name": "Noko  Kuratomo, University of Tsukuba"
            },
            {
                "name": "Hisham Elser Bilal  Salih, University of Tsukuba"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Support for people with visual impairment",
            "Stereophonic sound display",
            "Image Recognition"
        ],
        "subtype": "poster",
        "title": "Phonoscape: Auralization of Photographs using Stereophonic Auditory Icons",
        "type": "poster"
    },
    "upp1144": {
        "abstract": "Recent research has presented quadcopters to enable mid-air interaction. Using quadcopters to provide tactile feedback, navigation, or user input are the current scope of related work. However, most quadcopter steering systems are complicated to use for non-expert users or require an expensive tracking system for autonomous flying. Safety-critical scenarios require trained and expensive personnel to navigate quadcopters through crucial flight paths within narrow spaces. To simplify the input and manual operation of quadcopters, we present DroneCTRL, a tangible pointing device to navigate quadcopters. DroneCTRL resembles a remote control including optional visual feedback by a laser pointer and tangibility to improve the quadcopter control usability for non-expert users. In a preliminary user study, we compare the efficiency of hardware and software-based controller with DroneCTRL. Our results favor the usage of DroneCTRL with and without visual feedback to achieve more precision and accuracy.",
        "authors": [
            {
                "name": "Thomas  Kosch, LMU Munich"
            },
            {
                "name": "Markus  Funk, TU Darmstadt"
            },
            {
                "name": "Albrecht  Schmidt, LMU Munich"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Quadcopter",
            "Drone",
            "Controller",
            "Navigation"
        ],
        "subtype": "poster",
        "title": "DroneCTRL: A Tangible Remote Input Control for Quadcopters",
        "type": "poster"
    },
    "upp1145": {
        "abstract": "We present Resources2City Explorer (R2CE), a tool for representing ?le systems as interactive, walkable virtual cities. R2CE visualizes ?le systems based on concepts of spatial, 3D information processing. For this purpose, it extends the range of functions of conventional ?le browsers considerably. Visual elements in a city generated by R2CE represent (relations of) objects of the underlying ?le system. The paper describes the functional spectrum of R2CE and illustrates it by visualizing a sample of 940 ?les.",
        "authors": [
            {
                "name": "Attila  Kett, Goethe University Frankfurt"
            },
            {
                "name": "Giuseppe  Abrami, Goethe University Frankfurt"
            },
            {
                "name": "Alexander  Mehler, Goethe University"
            },
            {
                "name": "Christian  Spiekermann, Goethe University Frankfurt"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual Cities",
            "Virtual Reality",
            "3D Text-technologies",
            "Spatial Information Processing",
            ""
        ],
        "subtype": "poster",
        "title": "resources2City : Explorer A System for Generating Interactive Walkable Virtual Cities out of File Systems",
        "type": "poster"
    },
    "upp1148": {
        "abstract": "The muscles surrounding the human eye are capable of performing a wide range of expressions such as squinting, blinking, frowning, and raising eyebrows. This work explores the use of these ocular expressions to expand the input vocabularies of hands-free interactions. We conducted a series of user studies: 1) to understand which eye expressions users could consistently perform among all possible expressions, 2) to explore how these expressions can be used for hands-free interactions through a user-defined design process. Our study results showed that most participants could consistently perform 9 of the 18 possible eye expressions. Also, in the user define study the participants used the eye expressions to create hands-free interactions for the state-of-the-art augmented reality (AR) head-mounted displays.",
        "authors": [
            {
                "name": "Pin Sung  Ku, National Taiwan University"
            },
            {
                "name": "Te-Yen  Wu, National Taiwan University"
            },
            {
                "name": "Mike Y. Chen, National Taiwan University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "poster",
        "title": "EyeExpress: Expanding Hands-free Input Vocabulary using Eye Expressions",
        "type": "poster"
    },
    "upp1154": {
        "abstract": "In order to design serious games, attention needs to be paid to the target users. One important application of serious games is the design of games for older adults with dementia. Interfaces and activities in games designed for this group of users should be conducted by considering both the cognitive and physical limitations of these people, which may be challenging. We overcome these challenges by using the advantages of new head mounted display virtual reality (HMD-VR) technology and the knowledge of experts. The results of a preliminary three-week exercise involving participants with dementia shows that our design approach has been successful in achieving an interesting environment and could engage participants in the game.",
        "authors": [
            {
                "name": "Mahzar  Eisapour, University of Waterloo"
            },
            {
                "name": "Shi  Cao, University of Waterloo"
            },
            {
                "name": "Jennifer  Boger, University of Waterloo"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Virtual Reality",
            "Game Design",
            "Exergame",
            "Exercise",
            "Dementia",
            "Cognitive Impairment."
        ],
        "subtype": "poster",
        "title": "Game Design for Users with Constraint: Exergame for Older Adults with Cognitive Impairment",
        "type": "poster"
    },
    "upp1161": {
        "abstract": "Human-Robot Interaction (HRI) research in public spaces often encounters delays and restrictions due to several factors, including the need for sophisticated technology, regulatory approvals, and public or community support. To remedy these concerns, we suggest HRI can apply the core philosophy of Tactical Urbanism, a concept from urban planning, to catalyze HRI in public spaces, provide community feedback and information on the feasibility of future implementations of robots in the public, and also create social impact and forge connections with the community while spreading awareness about robots as a public resource. As a case study, we share tactics used and strategies followed to conduct a pop-up style study of 'A robotic mailbox to support and raise awareness about homelessness.' We discuss benefits and challenges of the pop-up approach and recommend using it to enable the social studies of HRI not only to match but to precede, the fast-paced technological advancement and deployment of robots.",
        "authors": [
            {
                "name": "Swapna  Joshi, Indiana University, Bloomington"
            },
            {
                "name": "Selma  Sabanovic, Indiana University, Bloomington"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            "Human-Robot Interaction in Public Spaces",
            "Research methods in HRI",
            "Pop-up interventions",
            "Tactical Urbanism",
            "Robots in Public Spaces",
            "Robots for Social Good",
            "Field research in HRI"
        ],
        "subtype": "poster",
        "title": "Pop-up Robotics: Facilitating HRI in Public Spaces",
        "type": "poster"
    },
    "uvs1016": {
        "abstract": "30 years after Weiser’s inspirational words on ubiquitous computing, I revisit one of the premises of that work. I propose a new era of self-sustainable computing through the development of computational materials that can be truly woven into thefabricofeverydaylifeandcreatedecadesofinspirationfor new researchers across a variety of disciplines. I will deﬁne and demonstrate some initial examples of computational materials and explain why self-sustainable computing provides a compelling vision for computing in a post-Moore’s Law world.",
        "authors": [
            {
                "name": "Gregory  Abowd, Georgia Institute of Technology"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "Vision",
        "title": "The Material for the 21st Century",
        "type": "Vision"
    },
    "uvs1022": {
        "abstract": "Suppose we could create interactive visualizations in the air during our day to day conversations? In the next few years, mixed reality technologies will make this possible. When that happens, how might language itself evolve? We describe a plan to help guide that evolution. If the capability to share visual communication under gestural control leads to a change in natural language itself, then future generations of children will grow up in a richer world, with powers of natural language-based communication that we now can only begin to envision. The resulting communicative power-up for coming generations may be as fundamental and paradigm changing as was the development of written language itself 5,000 years ago. ",
        "authors": [
            {
                "name": "Ken  Perlin, New York University"
            }
        ],
        "award": false,
        "hm": false,
        "keywords": [
            ""
        ],
        "subtype": "Vision",
        "title": "The Future Evolution of Language",
        "type": "Vision"
    }
}
